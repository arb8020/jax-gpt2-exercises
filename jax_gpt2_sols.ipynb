{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# in this notebook, we'll build up to a GPT in JAX"
      ],
      "metadata": {
        "id": "vxvbY4yUR0W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu1YSvvLRvWg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import List, Dict, Tuple, Callable, NamedTuple, Any, Optional\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### intro/what is a GPT\n",
        "\n"
      ],
      "metadata": {
        "id": "QOl2imGlfAKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT models Generate output, rather than predicting or classifying labels\n",
        "# typically, we might pass a model a pair (features, target)\n",
        "# where the model has to get the target 'correct' relative to the features\n",
        "# like guessing the correct house price given some data about the house\n",
        "# or guessing the right digit drawn from an image dataset\n",
        "\n",
        "# on the other hand:\n",
        "# passing an input to a GPT model like 'the man outside was'\n",
        "# the GPT model wouldn't produce a class label, or a number prediction\n",
        "# rather, it would generate the rest of the sentence like\n",
        "# 'the man outside was mowing his lawn'\n",
        "\n",
        "# GPT models learn to do this through 'pretraining'\n",
        "# instead of explicitly giving the model labeled examples like in the tasks above (features, target)\n",
        "# the model simply supervises itself, comparing an output it made to the actual data\n",
        "# this is usually called 'pretraining' rather than 'training' as its followed by a more classic training test\n",
        "# where the model actually has to classify or predict.\n",
        "# as an example, a GPT model might be pretrained on large chunks of text\n",
        "# and then asked to predict if a given imdb rating is 'positive' or 'negative'\n",
        "# the idea is that the model will become too specific to a given task if given labels\n",
        "# but we want a model that can generalize across tasks\n",
        "# this is clued in by the title of the GPT-2 paper 'language models are unsupervised task learners'\n",
        "\n",
        "# finally, the Transformer architecture is where the magic happens\n",
        "# there's a lot to cover there, so we'll motivate the choices we make step by step in what follows"
      ],
      "metadata": {
        "id": "nRL1c--iR4uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### why use jax?"
      ],
      "metadata": {
        "id": "AyyB5PRqe-hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: this motivation for jax could be better\n",
        "\n",
        "# in order to implement this ourselves\n",
        "# we will be using JAX\n",
        "# JAX is a library that is based on doing high performance math with arrays\n",
        "# arrays are really nice because we can represent very high dimensional data with them\n",
        "# if you've used numpy, a lot of the syntax will be very familiar\n",
        "# if a vector is a 1 dimensional object\n",
        "# a matrix is 2 dimensions\n",
        "# and everything beyond is a tensor\n",
        "# as an example, you can imagine a 3d tensor as a bunch of matrices stacked side by side into a cuboid type situation\n",
        "# (computerphile on tensors: https://www.youtube.com/watch?v=DfK83xEtJ_k)\n",
        "\n",
        "# jax is an autodifferentiating library\n",
        "# this means that it automatically can keep track of computations that happen to a given input\n",
        "# and automatically calculate partial derivatives\n",
        "# this is very useful for machine learning research\n",
        "# as backpropagation is how neural networks fit their parameters\n",
        "# 3blue1brown on backprop (https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n",
        "\n",
        "# backpropagation is frequently parallelized on 'batches' of input at a time\n",
        "# jax is nice in that it provides automatic function vectorization\n",
        "# it makes doing things like [f(i) for i in x] very efficient\n",
        "\n",
        "# in machine learning research\n",
        "# we also frequently have functions where the transformations don't really change\n",
        "# and we might want to compile those functions, rather than re-interpreting it each time\n",
        "# while python is very convenient for its extensive tooling and ease of writing\n",
        "# it doesnt natively have a way to compile and efficiently execute chunks of code\n",
        "# so jax introduces Just In Time compiling\n",
        "# making it easy to write performant code\n",
        "\n",
        "# ultimately, jax is just about expressing and composing transformations, or functions\n",
        "# https://www.youtube.com/watch?v=PAZTIAfaNr8\n",
        "# jit, vmap, and grad all work very easily with each other due to jax's functional style\n",
        "# and it makes the compilers life a lot easier too\n",
        "# code is also more easy to reproduce\n",
        "# as jax's functional style avoids implicit global states\n",
        "\n",
        "# let's import it now\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "fTV1F-AgaoRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### what goes into a GPT"
      ],
      "metadata": {
        "id": "684amOY2fE6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# back to the GPT, we'll go over some high level structure of what we need to make\n",
        "\n",
        "# as we mentioned before, a GPT generates some output, in our case text output\n",
        "# if we're doing a bunch of array based math\n",
        "# we'll need a way to turn strings into arrays of numbers\n",
        "# we call this a 'tokenizer'\n",
        "# it breaks up a string into different pieces in a way that makes sense\n",
        "# and each piece gets its own id\n",
        "# ultimately, we want a model that generates text\n",
        "# machine learning models don't really use words\n",
        "# so we'll need a way to go from text: str -> tokens: jax.Array, using some learned parameters\n",
        "# this introduces a model configuration setting: 'vocabulary size'\n",
        "# which is just the total number of tokens the model knows about\n",
        "# we'll call it vocab_size from now on\n",
        "\n",
        "# now, our GPT needs to take in that tokens: jax.Array, and output some generated_tokens: jax.Array\n",
        "# if we take in the tokens 'hello world', our sequence length is 11\n",
        "# so our input is of shake (seq_len, vocab_size)\n",
        "\n",
        "# but it doesn't generate all the tokens at once\n",
        "# it'll take the input, and predict the next token that should come after that input\n",
        "# continuing this until it either decides to stop, or until we cut it off\n",
        "# the next token prediction is usually a distribution of values\n",
        "# one value for each possible next token\n",
        "# we call that raw output 'logits'\n",
        "# since its just for the next token, the logits are of size (vocab_size)\n",
        "\n",
        "# from this distribution of logits\n",
        "# we need a way to choose the single token, or sample from the distribution\n",
        "# the simplest way of doing this is to just pick the highest value\n",
        "# so we'll take in the logits of shape (vocab_size) and output a single number\n",
        "\n",
        "# lastly, we'll need to implement the unsupervised training for the model\n",
        "# we have to give the model a way to understand how good/bad its token prediction was\n",
        "# also known as a loss function\n",
        "# and update its parameters based on the the gradient with respect to that loss function\n",
        "\n",
        "# we'll implement basic versions of all of the above, and build on them over time\n",
        "\n",
        "# to make this faster, we build it so it can run in parallel\n",
        "# we can do multiple sequences, of shape (batch_size, seq_len, vocab_size)\n",
        "# so the logit output becomes (batch_size, vocab_size)\n",
        "# and the final output is (batch_size,)\n",
        "# jax's automatic vectorization we discussed earlier makes this very convenient\n"
      ],
      "metadata": {
        "id": "Py9aMFpbR5i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### basic tokenizer"
      ],
      "metadata": {
        "id": "nFZ4LPZn7cHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: could break this up into initial tokenizer -> with special chars\n",
        "\n",
        "# for our initial tokenizer, we can use python's ord() and chr()\n",
        "# to turn chars into integers, and vice versa\n",
        "# the ascii printable characters are from 32-126\n",
        "# so we'll define a 'char_to_idx' and an 'idx_to_char' function\n",
        "# this will make our lives easier by mapping into the range 0-94\n",
        "# as a default, we can return 0 if ord(x) is outside of the printable character idxs\n",
        "# and return ('\\n') if we get an index we don't know about\n",
        "\n",
        "\"\"\"\n",
        "def char_to_idx(char: int) -> int:\n",
        "    return\n",
        "\n",
        "def idx_to_char(index: int) -> int:\n",
        "    return\n",
        "\n",
        "print(char_to_idx(ord('a')))\n",
        "print(chr(idx_to_char(char_to_idx(ord('a')))))\n",
        "\"\"\"\n",
        "\n",
        "def char_to_idx(char: int) -> int:\n",
        "    if 32 <= char <= 126:\n",
        "        return char - 32\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def idx_to_char(index: int) -> int:\n",
        "    if 0 <= index <= 94:\n",
        "        return index + 32\n",
        "    else:\n",
        "        return ord('\\n')\n",
        "\n",
        "\n",
        "print(char_to_idx(ord('a')))\n",
        "print(chr(idx_to_char(char_to_idx(ord('a')))))"
      ],
      "metadata": {
        "id": "r6M3Waetr04B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, we make the tokenizer\n",
        "# we'll create a namedtuple object, to easily organize our tokenizer\n",
        "\n",
        "\"\"\"\n",
        "class Tokenizer(NamedTuple):\n",
        "    encode: Callable\n",
        "    decode: Callable\n",
        "\n",
        "# use char_to_idx/idx_to_char from above with a list comprehension\n",
        "\n",
        "def tokenizer_encode(text: str) -> jax.Array:\n",
        "    return ...\n",
        "\n",
        "def tokenizer_decode(ids: jax.Array) -> str:\n",
        "    return ...\n",
        "\n",
        "tokenizer = Tokenizer(encode=encode, decode=decode)\n",
        "# Test the functions\n",
        "print('hello')\n",
        "print(tokenizer.encode('hello'))\n",
        "print(tokenizer.decode(tokenizer.encode('hello')))\n",
        "\n",
        "print('1.\\n2.')\n",
        "print(tokenizer.encode('1.\\n2.'))\n",
        "print(tokenizer.decode(tokenizer.encode('1.\\n2.')))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class Tokenizer(NamedTuple):\n",
        "    encode: Callable\n",
        "    decode: Callable\n",
        "\n",
        "def encode(text: str) -> jax.Array:\n",
        "    return jnp.array([char_to_idx(ord(c)) for c in text])\n",
        "\n",
        "def decode(ids: jax.Array) -> str:\n",
        "    return ''.join([chr(idx_to_char(int(id))) for id in ids])\n",
        "\n",
        "tokenizer = Tokenizer(encode=encode, decode=decode)\n",
        "# Test the functions\n",
        "print('hello')\n",
        "print(tokenizer.encode('hello'))\n",
        "print(tokenizer.decode(tokenizer.encode('hello')))\n",
        "\n",
        "print('1.\\n2.')\n",
        "print(tokenizer.encode('1.\\n2.'))\n",
        "print(tokenizer.decode(tokenizer.encode('1.\\n2.')))\n",
        "\n",
        "# notice that our tokenizer does not handle the \\n token!"
      ],
      "metadata": {
        "id": "oQKiDo0Qq2XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### bigram model"
      ],
      "metadata": {
        "id": "F3Sn5SsVvuKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we'll implement the most basic possible model\n",
        "# it should perform a lookup from one vocabulary word, to some random logits, of size (batch_size ,vocab_size)\n",
        "# suppose we had a vocab size of 3: [a,b,c]\n",
        "# our matrix basically looks like\n",
        "#      a     b     c\n",
        "# a  0.1   0.4   0.5\n",
        "# b  0.9  0.02  0.08\n",
        "# c  0.8   0.1   0.1\n",
        "\n",
        "# so if we have token b, we would usually predict token a, with 90% probability\n",
        "# the above values are probabilities for simplicity\n",
        "# but in actuality will be generated from a normal distribution (0,1) * scaling_factor"
      ],
      "metadata": {
        "id": "dmPFT_8GxMoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first we'll create a struct to hold configuration parameters\n",
        "# an initialization function to initialize parameters\n",
        "# and a forward pass of the model, taking in some input and creating outpu\n",
        "\n",
        "# struct\n",
        "class ModelConfig(NamedTuple):\n",
        "    vocab_size: int"
      ],
      "metadata": {
        "id": "O6ODLcoN0VhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to initialize parameters\n",
        "# jax wants us to pass in a jax.random.PRNGKey()\n",
        "# jax uses this sort of like the way a seed works\n",
        "# but rather than defining a global seed, we get to explicitly pass it to our random functions\n",
        "# https://jax.readthedocs.io/en/latest/random-numbers.html#explicit-random-state\n",
        "# we'll store our parameters in a dictionary, and call the matrix 'token_table'\n",
        "# it should have shape (vocab_size, vocab_size), to have a mapping from each token to each other token in the vocab\n",
        "\n",
        "\"\"\"\n",
        "def initialize_bigram_params(model_config: ModelConfig, key=jax.random.PRNGKey(0)) -> Dict:\n",
        "    vocab_size = model_config.vocab_size\n",
        "    weights = ... # initialize with jax.random.normal\n",
        "    model_params = {'token_table': }\n",
        "    return model_params\n",
        "\n",
        "bigram_config = ModelConfig(vocab_size = 95)\n",
        "bigram_params = initialize_bigram_params(bigram_config)\n",
        "bigram_params['token_table'].shape\n",
        "\"\"\"\n",
        "\n",
        "def initialize_bigram_params(model_config: ModelConfig, key=jax.random.PRNGKey(0)) -> Dict:\n",
        "    vocab_size = model_config.vocab_size\n",
        "    weights = jax.random.normal(key, (vocab_size, vocab_size))\n",
        "    model_params = {'token_table': weights}\n",
        "    return model_params\n",
        "\n",
        "bigram_config = ModelConfig(vocab_size = 95)\n",
        "bigram_params = initialize_bigram_params(bigram_config)\n",
        "bigram_params['token_table'].shape"
      ],
      "metadata": {
        "id": "ZSJMCagF0NZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and now we write the forward pass\n",
        "# as discussed above, we'll use the token table\n",
        "# to figure out what our prediction for the next token should be\n",
        "# we'll be taking in a vector of shape (batch_size, sequence_length,)\n",
        "# take the last token as our key\n",
        "# keying into the token_table will return a vector of size (batch_size, vocab_size)\n",
        "# and finally, we should return a vector of size (batch_size, 1, vocab_size)\n",
        "\n",
        "\"\"\"\n",
        "def bigram_model(model_params: Dict, model_config: ModelConfig, tokens: jax.Array): # (seq_len,) -> (1,vocab_size)\n",
        "    token_table = ...\n",
        "    last_token_id = ...\n",
        "    logits = ...\n",
        "    reshaped_logits = ...\n",
        "    return reshaped_logits\n",
        "\n",
        "\n",
        "logits = bigram_model(bigram_params, bigram_config, tokenizer.encode('hello'))\n",
        "logits.shape # you can use this to debug!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def bigram_model(model_params: Dict, model_config: ModelConfig, tokens: jax.Array): # (seq_len,) -> (1,vocab_size)\n",
        "    token_table = model_params['token_table']\n",
        "    last_token_id = tokens[-1]\n",
        "    logits = token_table[last_token_id]\n",
        "    reshaped_logits = logits.reshape((1, model_config.vocab_size))\n",
        "    return reshaped_logits\n",
        "\n",
        "logits = bigram_model(bigram_params, bigram_config, tokenizer.encode('hello'))\n",
        "logits.shape # you can use this to debug!"
      ],
      "metadata": {
        "id": "kfllkl5T1PiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sampling"
      ],
      "metadata": {
        "id": "7NWbxmUSvzF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we take this raw logit output\n",
        "# and implement a sample function\n",
        "# which should take in the model output of shape (batch_size, vocab_size)\n",
        "# and return values, of shape (batch_size,)\n",
        "\n",
        "# we can think of the next token prediction as similar to a classification task\n",
        "# suppose we had to pick between [red, green, blue] in some computer vision model\n",
        "# the correct 'label' might be [1, 0, 0]\n",
        "# a classification model usually compares the label to some probability distribution output\n",
        "# as an example, our model output might be one of [0.9, 0.02, 0.08] or [0.4, 0.3, 0.3]\n",
        "# while the maximum value for both is correct\n",
        "# its clear that one model potentially performs better at the task\n",
        "# as its probability distribution is in some sense 'closer' to the true distribution\n",
        "\n",
        "# but we need don't have the probabilities yet\n",
        "# our model, in the above example, might be outputting something like [5, 3, 2]\n",
        "# how do we turn this into a probability?"
      ],
      "metadata": {
        "id": "xc9h2KUn5WZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets call that initial output f(x), with values f(x_i)\n",
        "# we need to create a probability distribution p(x)\n",
        "# but we dont want to lose any of the information from f(x)\n",
        "# by the principle of maximum entropy\n",
        "# the best representation of some state of knowledge is the probability distribution that maximizes entropy\n",
        "# in this context, entropy is the expected value of 'surprise'\n",
        "\n",
        "# the 'surprise' of some outcome can actually be mathematically defined\n",
        "# to develop intuition about this, suppose you had 100 red balls and 2 blue balls in a box\n",
        "# drawing a blue ball out is more 'surprising' than a red ball\n",
        "# so the surprise might be 1/p\n",
        "# but if p is 0 this breaks, so we just add a log transform\n",
        "# thus, the rigorous probability definition of surprise is log(1/p)\n",
        "# and entropy is E = sum (p(x) * log(1/p(x)))\n",
        "# this can be simplified into E = -sum(p(x) * log(p(x))) (see if you can do it yourself)\n",
        "\n",
        "# so we can set up a constrained optimization problem\n",
        "# to figure out a function that can map f(x) into the best p(x)\n",
        "# our objective function is maximizing E = -sum(p(x) * log(p(x)))\n",
        "# our constraints are that\n",
        "# 1. sum(p(x)) = 1\n",
        "# 2. p_i >= 0\n",
        "\n",
        "# using lagrange multipliers, we can solve the above to get\n",
        "# p_i = e^{beta * x_i}/sum(e^{beta * x_j})\n",
        "# for writing the softmax, we'll remove beta\n",
        "# but remember that we can scale the logits before doing the softmax\n",
        "# this beta constant is frequently rewritten as 1/T, where T is temperature\n",
        "# we call it that because of the boltzmann distribution in statistical mechanics\n",
        "# i dont know enough about that to explain it but: https://en.wikipedia.org/wiki/Boltzmann_distribution\n",
        "\n",
        "\"\"\"\n",
        "def softmax(x: jnp.array) -> jnp.array:\n",
        "    return\n",
        "\n",
        "x1 = jnp.array([1.2, 2, -4, 0.0])\n",
        "x2 = jnp.array([1.2, 2000, -4000, 0.0])\n",
        "\n",
        "print(softmax(x1))\n",
        "print(softmax(x2))\n",
        "\"\"\"\n",
        "\n",
        "def softmax(x: jax.Array) -> jax.Array:\n",
        "    exp_x = jnp.exp(x)\n",
        "    return exp_x / jnp.sum(exp_x)\n",
        "\n",
        "x1 = jnp.array([1.2, 2, -4, 0.0])\n",
        "x2 = jnp.array([1.2, 2000, -4000, 0.0])\n",
        "\n",
        "print(softmax(x1))\n",
        "print(softmax(x2))"
      ],
      "metadata": {
        "id": "IiTu-ifI6FtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you might have noticed we have a bit of instability with this computation\n",
        "# e^x where x is a really big number can easily overflow\n",
        "# but e^-x will always be between 0 and 1\n",
        "# so we want to see if we can shift all of our e^x such that x is, at most, 0\n",
        "# suppose we subtracted some constant c from all of our x_i (from the initial f(x))\n",
        "# mathematically, our function is invariant to any shifts (can you prove this)\n",
        "# so we can just subtract the largest value from every value\n",
        "# to get a numerically stable softmax\n",
        "\n",
        "\"\"\"\n",
        "def stable_softmax(x: jnp.array) -> jnp.array:\n",
        "    return\n",
        "\n",
        "x1 = jnp.array([1.2, 2, -4, 0.0])\n",
        "x2 = jnp.array([1.2, 2000, -4000, 0.0])\n",
        "\n",
        "print(stable_softmax(x1))\n",
        "print(stable_softmax(x2))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def stable_softmax(x: jax.Array) -> jax.Array:\n",
        "    exp_x = jnp.exp(x - jnp.max(x))\n",
        "    return exp_x / jnp.sum(exp_x)\n",
        "\n",
        "x1 = jnp.array([1.2, 2, -4, 0.0])\n",
        "x2 = jnp.array([1.2, 2000, -4000, 0.0])\n",
        "\n",
        "print(stable_softmax(x1))\n",
        "print(stable_softmax(x2))"
      ],
      "metadata": {
        "id": "PAv8Koe07vtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to set up our generation\n",
        "# we'll need to pass in our model parameters, config, forward pass\n",
        "# to make selecting the forward pass easier, we'll set up a dictionary MODEL_DICT\n",
        "\n",
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "}\n",
        "\n",
        "# our generate function should use\n",
        "# model params/config/fwd pass\n",
        "# prompt tokens\n",
        "# max tokens to generate\n",
        "# temperature (as previously discussed)\n",
        "# and a jax key\n",
        "# we'll be using the jax.random.split as discussed in # https://jax.readthedocs.io/en/latest/random-numbers.html#explicit-random-state\n",
        "\n",
        "# while we haven't finished generating tokens\n",
        "# get the logits for the current sequence\n",
        "# get the last token's logits\n",
        "# scale the logits with temperature\n",
        "# sample with jax.random.categorical to get the next token (we can use shape (1,), not parallelizing generations)\n",
        "# add to the generated tokens to return (you can use jnp.concatenate)\n",
        "# add to the tokens we use for the prompt\n",
        "# repeat\n",
        "\n",
        "\"\"\"\n",
        "def generate(params: Dict, model_config: ModelConfig, model_name: str, prompt_tokens: jax.Array, max_new: int, temp=1, key=jax.random.PRNGKey(0)):\n",
        "    gen_tokens = jnp.array([], dtype=jnp.int32)\n",
        "    cur_pos = 0\n",
        "\n",
        "    while cur_pos < max_new:\n",
        "        # may want to split your key on each generation\n",
        "        key, subkey = ...\n",
        "        logits = ...\n",
        "        last_token_logit = ...\n",
        "        scaled_logit = ...\n",
        "        next_token = ...\n",
        "        gen_tokens = ...\n",
        "        tokens = ...\n",
        "        cur_pos += 1\n",
        "\n",
        "    return gen_tokens\n",
        "\n",
        "prompt = 'hello?'\n",
        "\n",
        "model_name = 'bigram_model'\n",
        "tokenized_prompt = jnp.array(tokenizer.encode(prompt), dtype=jnp.int32)\n",
        "generated_tokens = generate(bigram_params, bigram_config, model_name, tokens=tokenized_prompt, max_new=10, temp=0.8, key=jax.random.PRNGKey(0))\n",
        "generated_text = tokenizer.decode(generated_tokens)\n",
        "print(prompt + generated_text)\n",
        "\"\"\"\n",
        "\n",
        "def generate(params: Dict, model_config: ModelConfig, model_name: str, tokens: jax.Array, max_new: int, temp=1, key=jax.random.PRNGKey(0)):\n",
        "    gen_tokens = jnp.array([], dtype=jnp.int32)\n",
        "    cur_pos = 0\n",
        "\n",
        "    while cur_pos < max_new:\n",
        "        key, subkey = jax.random.split(key, 2)\n",
        "        logits = MODEL_DICT[model_name](params, model_config, tokens)\n",
        "        last_token_logit = logits[-1:]\n",
        "        scaled_logit = last_token_logit / temp\n",
        "        next_token = jax.random.categorical(subkey, scaled_logit, shape=(1,))\n",
        "        gen_tokens = jnp.concatenate((gen_tokens, next_token))\n",
        "        tokens = jnp.concatenate((tokens, next_token))\n",
        "        cur_pos += 1\n",
        "\n",
        "    return gen_tokens\n",
        "\n",
        "prompt = 'hello?'\n",
        "\n",
        "model_name = 'bigram_model'\n",
        "tokenized_prompt = jnp.array(tokenizer.encode(prompt), dtype=jnp.int32)\n",
        "generated_tokens = generate(bigram_params, bigram_config, model_name, tokens=tokenized_prompt, max_new=10, temp=0.8, key=jax.random.PRNGKey(0))\n",
        "generated_text = tokenizer.decode(generated_tokens)\n",
        "print(prompt + generated_text)"
      ],
      "metadata": {
        "id": "etHIGm714xpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loss"
      ],
      "metadata": {
        "id": "raPvTu49v76u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# turns out, this model is pretty weak\n",
        "# before we go about improving it, we need to measure how well/poorly its doing\n",
        "# our model outputs logits, which get softmaxed into some distribution q\n",
        "# for each training datapoint, we want to know how surprising the real distribution is\n",
        "# for example, if our guess q has 0.9 for the token !  and 0.002 for ?, but the actual label is ?\n",
        "# then we were pretty wrong\n",
        "# turns out we can basically just use the entropy equation from before\n",
        "# but rather than using\n",
        "# E = -sum(p(x) * log(p(x)))\n",
        "# we just replace one of the p(x) distributions with our model output distribution q(x)\n",
        "# C = -sum(p(x) * log(q(x)))\n",
        "# this is known as 'cross-entropy'\n",
        "# note that we choose to put the log transform on our q distribution bc log(0) is not stable\n",
        "\n",
        "# cross_entropy_loss should take in logits, of shape (batch_size, vocab_size)\n",
        "# the targets, should be (batch_size,)\n",
        "\n",
        "# we know that we have vocab_size classes\n",
        "# so we reshape the targets using one hot encoding, into (batch_size, vocab size) as well\n",
        "\n",
        "# one hot encoding would take some id of 3, for a vocab size 5 into [0,0,0,1,0]\n",
        "\n",
        "# after that, take the log softmax because we log transformed p(x) above\n",
        "# and then the loss is just the equation we derived above: the negated sum of the expected surprise\n",
        "\n",
        "# we use axis=-1 because we want to sum over the vocab_size in the shape\n",
        "\n",
        "# finally, we take the mean of this value across the batch size\n",
        "# to output our final loss value (as a scalar)\n",
        "\n",
        "\"\"\"\n",
        "def cross_entropy_loss(logits: jnp.ndarray, targets: jnp.ndarray) -> jnp.ndarray:\n",
        "\n",
        "    num_classes = ...\n",
        "    onehot_targets = ...\n",
        "    log_probs = ...\n",
        "    loss = ...\n",
        "\n",
        "    mean_loss = ...\n",
        "\n",
        "    return mean_loss\n",
        "\n",
        "text = 'hello worl'\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "target = tokenizer.encode('d')\n",
        "\n",
        "logits = bigram_model(bigram_params, bigram_config, tokenized_text)\n",
        "cross_entropy_loss(logits, target)\n",
        "\"\"\"\n",
        "\n",
        "def cross_entropy_loss(logits: jnp.ndarray, targets: jnp.ndarray) -> jnp.ndarray:\n",
        "\n",
        "    num_classes = logits.shape[-1]\n",
        "    onehot_targets = jax.nn.one_hot(targets, num_classes)\n",
        "    log_probs = jax.nn.log_softmax(logits)\n",
        "    loss = -jnp.sum(onehot_targets * log_probs, axis=-1)\n",
        "\n",
        "    mean_loss = jnp.mean(loss)\n",
        "\n",
        "    return mean_loss\n",
        "\n",
        "text = 'hello worl'\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "target = tokenizer.encode('d')\n",
        "\n",
        "logits = bigram_model(bigram_params, bigram_config, tokenized_text)\n",
        "cross_entropy_loss(logits, target)"
      ],
      "metadata": {
        "id": "9B4qK6QNx5Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### backpropagation and optimizers (grad/pytree intro)"
      ],
      "metadata": {
        "id": "5-B1TCAPHGdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now that we can compute how bad our model is\n",
        "# we should try to update its parameters\n",
        "\n",
        "# suppose our model had 2 parameters, x and y\n",
        "# and the loss function was some z axis\n",
        "# we could plot the surface of the loss as a function of x and y\n",
        "# and try to find the minimum value of it\n",
        "\n",
        "# unfortunately, we won't know what the surface looks like at all points\n",
        "# we can only compute it for some pair (x,y)\n",
        "# so how can we find the minimum value?\n",
        "\n",
        "# one way to motivate this is to imagine a blind man on the loss surface\n",
        "# trying to get to the bottom\n",
        "# at each point on the loss surface\n",
        "# the blind man can feel out how steep the ground he stands on is\n",
        "# and follow the slope\n",
        "\n",
        "# if we take the gradient of the loss function with respect to our parameters\n",
        "# we can basically do the same thing with our model\n",
        "# for each of our parameters\n",
        "# we update the parameter by taking a step 'downhill'\n",
        "# typically, we take a step in the direction by multiplying the gradient by some 'learning rate'\n",
        "\n",
        "parameter = 2\n",
        "grad = 0.2\n",
        "learning_rate = 0.01\n",
        "new_parameter = parameter - learning_rate * grad\n",
        "new_parameter"
      ],
      "metadata": {
        "id": "X8zIHjGZzzYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as previously mentioned, jax is an autodiff library\n",
        "# so we can automatically calculate the gradient with respect to some loss function\n",
        "# basically jax applies a tracer object to all of its arguments\n",
        "# the tracer records all the operations that happen to it\n",
        "# and jax creates a Jaxpr out of it\n",
        "# as an example, lets look at gelu, the activation function used in GPT-2\n",
        "from jax import grad\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + jnp.tanh(jnp.sqrt(2 / jnp.pi) * (x + 0.044715 * jnp.power(x, 3))))\n",
        "\n",
        "jax.make_jaxpr(gelu)(3.0)"
      ],
      "metadata": {
        "id": "xtHP_eXLzTZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# when computing the gradient\n",
        "# jax traverses this intermediate representation to calculate the derivative\n",
        "\n",
        "\n",
        "slope = 2.0\n",
        "intercept = -0.5\n",
        "\n",
        "x = 1.5\n",
        "\n",
        "pred = slope * x + intercept\n",
        "\n",
        "true = 4.0\n",
        "\n",
        "mse = lambda y_hat, y: (y_hat - y)**2\n",
        "\n",
        "print(jax.make_jaxpr(mse)(pred,true)) # c = a - b, d = c**2\n",
        "\n",
        "value, grad = jax.value_and_grad(mse)(pred, true) # value and grad gives us both the value and gradient\n",
        "value, grad"
      ],
      "metadata": {
        "id": "Vy9jg7aWzx2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see what this looks like for a training step here\n",
        "\n",
        "\"\"\"\n",
        "text = 'hello worl'\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "target = tokenizer.encode('d')\n",
        "\n",
        "def model_loss(model_params, model_config, model_name, tokens, target):\n",
        "    logits = ...\n",
        "    loss = ...\n",
        "    return loss\n",
        "\n",
        "# use partial to set up the loss function, without passing in params yet\n",
        "partial_loss_fn = partial(...)\n",
        "\n",
        "# view jaxpr\n",
        "jaxpr = jax.make_jaxpr(jax.value_and_grad(partial_loss_fn))(bigram_params)\n",
        "print(jaxpr)\n",
        "\n",
        "loss, grads = ...\n",
        "\n",
        "print(f'loss: {loss}')\n",
        "print(f'grad: {grad}')\n",
        "\n",
        "lr = 1e-4\n",
        "new_bigram_params = ...\n",
        "\"\"\"\n",
        "\n",
        "text = 'hello worl'\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "target = tokenizer.encode('d')\n",
        "\n",
        "def model_loss(model_params, model_config, model_name, tokens, target):\n",
        "    logits = MODEL_DICT[model_name](model_params, model_config, tokens)\n",
        "    loss = cross_entropy_loss(logits, target)\n",
        "    return loss\n",
        "\n",
        "# use partial to set up the loss function, without passing in params yet\n",
        "partial_loss_fn = partial(model_loss, model_config=bigram_config, model_name='bigram_model', tokens=tokenized_text, target=target)\n",
        "\n",
        "# make jaxpr\n",
        "jaxpr = jax.make_jaxpr(jax.value_and_grad(partial_loss_fn))(bigram_params)\n",
        "print(jaxpr)\n",
        "\n",
        "loss, grads = jax.value_and_grad(partial_loss_fn)(bigram_params)\n",
        "\n",
        "print(f'loss: {loss}')\n",
        "print(f'grad: {grad}')\n",
        "\n",
        "lr = 1e-4\n",
        "new_bigram_params = bigram_params['token_table'] - grad * lr"
      ],
      "metadata": {
        "id": "qxFhBwn92bkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we want a scalable way to apply this new_param = param - lr * grad to all of our parameters\n",
        "# in a way that maintains the structure that we passed our params in\n",
        "\n",
        "# jax's pytrees are container-like structures\n",
        "# that are built out of other python container structures (lists, tuples, dicts, etc)\n",
        "# its leaves are anything that isn't a pytree, like a jax array\n",
        "\n",
        "example_trees = [\n",
        "    [1, {'k1': 2, 'k2': (3, 4)}, jnp.array([1,3])],\n",
        "    {'a': 2, 'b': (7, 2), 'c': jnp.array([4,5,6])},\n",
        "    jnp.array([1, 2, 3]),\n",
        "    [1, 'a', 17.],\n",
        "    (1, (2, 3),None),\n",
        "]\n",
        "\n",
        "# TODO: turn this into a test\n",
        "\n",
        "for pytree in example_trees:\n",
        "  leaves = jax.tree.leaves(pytree)\n",
        "  print(f\"{repr(pytree)} has {len(leaves)} leaves: {leaves}\")"
      ],
      "metadata": {
        "id": "Gwndbk4q6UY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can apply a function to all of a trees leaves with jax.tree.map\n",
        "# while maintaining the original structure\n",
        "# jax.tree.map(func, tree)\n",
        "\n",
        "tree = [-17, 3, 8, [4, -6, 7,[1, 2, -3], 4]]\n",
        "jax.tree.map(lambda x: x**2, tree)"
      ],
      "metadata": {
        "id": "DwjFJ-J96m6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use jax.tree.map to implement the update step of stochastic gradient descent\n",
        "# assume that we already have the gradients\n",
        "\n",
        "class OptConfig(NamedTuple):\n",
        "    lr: int\n",
        "    opt_init: Callable\n",
        "    opt_update: Callable\n",
        "\n",
        "# we'll keep track of our optimizer's state using a dictionary, similar to the model params dictionary\n",
        "\"\"\"\n",
        "class OptConfig(NamedTuple):\n",
        "    lr: int\n",
        "    opt_init: Callable\n",
        "    opt_update: Callable\n",
        "\n",
        "def init_sgd_state(model_params: Dict, opt_config: OptConfig) -> Dict:\n",
        "    state = {'step': 0,\n",
        "             'lr': }\n",
        "    return state\n",
        "\n",
        "def sgd_update(model_params: Dict, grads: Dict, opt_state: Dict, opt_config: OptConfig) -> Tuple[Dict, Dict]:\n",
        "    new_opt_state = {\n",
        "        'step': ,\n",
        "    }\n",
        "    new_params = ... # jax tree map and a lambda will come in handy here\n",
        "    return new_params, new_opt_state\n",
        "\"\"\"\n",
        "\n",
        "class OptConfig(NamedTuple):\n",
        "    lr: int\n",
        "    opt_init: Callable\n",
        "    opt_update: Callable\n",
        "\n",
        "def init_sgd_state(model_params: Dict, opt_config: OptConfig) -> Dict:\n",
        "    state = {'step': 0, 'lr': opt_config.lr}\n",
        "    return state\n",
        "\n",
        "def sgd_update(model_params: Dict, grads: Dict, opt_state: Dict, opt_config: OptConfig) -> Tuple[Dict, Dict]:\n",
        "    new_opt_state = {\n",
        "        'step': opt_state['step'] + 1,\n",
        "    }\n",
        "    new_params = jax.tree_map(lambda p, g: p - opt_config.lr * g, model_params, grads)\n",
        "    return new_params, new_opt_state"
      ],
      "metadata": {
        "id": "8_dws6EM5KI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can write a training step\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "sgd_config = ...\n",
        "opt_state = ...\n",
        "\n",
        "text = 'hello worl'\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "target = tokenizer.encode('d')\n",
        "\n",
        "loss, grads = ...\n",
        "\n",
        "new_params, new_opt_state = ...\n",
        "new_params\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "sgd_config = OptConfig(lr=1e-2, opt_init=init_sgd_state, opt_update=sgd_update)\n",
        "opt_state = sgd_config.opt_init(bigram_params, sgd_config)\n",
        "\n",
        "text = 'hello worl'\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "target = tokenizer.encode('d')\n",
        "\n",
        "loss, grads = jax.value_and_grad(model_loss)(bigram_params, bigram_config, 'bigram_model', tokenized_text, target)\n",
        "\n",
        "new_params, new_opt_state = sgd_update(bigram_params, grads, opt_state, sgd_config)\n",
        "new_params"
      ],
      "metadata": {
        "id": "tjf9tt2Vnsi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### optimizations for larger data (intro to jit/vmap)"
      ],
      "metadata": {
        "id": "4v-BAR8XwUag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now that we've created a model, a loss, and an optimizer\n",
        "# we'll need data to train the model, and update its parameters\n",
        "# the tiny shakespeare dataset has all of shakespeare's works\n",
        "# we'll download it from hugging face datasets library"
      ],
      "metadata": {
        "id": "I5aUN7NCIVo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w5jSt0Z1AHdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('tiny_shakespeare')\n",
        "dataset"
      ],
      "metadata": {
        "id": "xHxrmUaI_PsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so the dataset has a train, validation, and test set\n",
        "# there's just one row for each dataset\n",
        "# fill out the following to load in the different datasets\n",
        "\n",
        "\"\"\"\n",
        "train_text = dataset['train']['text'][0]\n",
        "val_text = ...\n",
        "test_text = ...\n",
        "len(train_text), len(val_text), len(test_text)\n",
        "\"\"\"\n",
        "\n",
        "train_text = dataset['train']['text'][0]\n",
        "val_text = dataset['validation']['text'][0]\n",
        "test_text = dataset['test']['text'][0]\n",
        "\n",
        "len(train_text), len(val_text), len(test_text)"
      ],
      "metadata": {
        "id": "aIUtHHMoAOil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the amount of data we're about to work with\n",
        "# we'll need to speed up our tokenizing functions\n",
        "# if you remember when we discussed jax's just in time compilation, here's a great place to use it\n",
        "\n",
        "# let's look at a quick example of how that works"
      ],
      "metadata": {
        "id": "9MmPE4lqwJcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jit, just in time compiling, is one of the key reasons to use jax\n",
        "# just in time compiling compiles a function during runtime, before it executes\n",
        "# it's one of the key reasons to use JAX, providing speedups to commonly used functions\n",
        "# suppose for example, the gelu activation function we defined earlier\n",
        "\n",
        "x = jnp.arange(1000000)\n",
        "%timeit gelu(x).block_until_ready()"
      ],
      "metadata": {
        "id": "TknKdM38whin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function will execute every time its called with python\n",
        "# but if we wrap it with jax.jit ...\n",
        "\n",
        "gelu_jit = jax.jit(gelu)\n",
        "gelu_jit(x).block_until_ready()\n",
        "%timeit gelu_jit(x).block_until_ready()"
      ],
      "metadata": {
        "id": "13JeJ0dkwi_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that its a lot faster!\n",
        "# if you noticed, the function took longer to start running\n",
        "# but each run was almost an order of magniture faster\n",
        "# this is because jax compiles the function the first time we run it\n",
        "# by using the tracer objects/method we saw in the grad section\n",
        "# and optimizes it for the hardware it will run on, like a GPU/TPU\n",
        "# this adds some overhead for the functions we jit compile\n",
        "# but it allows every subsequent run to be significantly faster\n",
        "# this is really great for neural networks, which can apply activation functions, or parameter updates thousands of times"
      ],
      "metadata": {
        "id": "4qfJm4A4wk-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notice that this will skip over some steps with side effects, like print statements\n",
        "def print_gelu(x):\n",
        "    print('x: ', x)\n",
        "    return gelu(x)\n",
        "\n",
        "# also notice that the x that gets printed is a traced object!\n",
        "jax.make_jaxpr(print_gelu)(3.0)\n",
        "\n",
        "# to get the desired printing behavior, use jax.debug.print() instead"
      ],
      "metadata": {
        "id": "ylGjdTtDwt5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jax's jax arrays are specifically immutable objects, to comply with the tracing we saw above\n",
        "# so jit and the other functions really want you to use jax arrays\n",
        "# let's say that you wanted to create a jnp.sum() that allowed any inputs\n",
        "# by casting it into an array inside the function\n",
        "\n",
        "def permissive_sum(x):\n",
        "  return jnp.sum(jnp.array(x))\n",
        "\n",
        "x = list(range(10))\n",
        "jax.make_jaxpr(permissive_sum)(x) # what do you expect this to output?"
      ],
      "metadata": {
        "id": "d0ZBjJ5lwskK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# since lists are mutable, jax considers every element of a python list to be its own element to get traced\n",
        "# so make sure to explicitly have your inputs be jax arrays\n",
        "\n",
        "x = list(range(10))\n",
        "jax.make_jaxpr(jnp.sum)(jnp.array(x)) # what will this output?"
      ],
      "metadata": {
        "id": "ToDmVkp3ww_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see how JIT interacts with global variables\n",
        "g = 10\n",
        "def func(x):\n",
        "    return x + 5 + g\n",
        "\n",
        "func_jit = jax.jit(func)\n",
        "print(func_jit(10))\n",
        "g = 20\n",
        "print(func_jit(10)) # what will this output?\n",
        "jax.make_jaxpr(func_jit)(10)"
      ],
      "metadata": {
        "id": "8hL2PzGDwyjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# because of what we talked about with the tracing\n",
        "# JIT will compile the function with the operation that it observed the first time the function ran\n",
        "# with g = 10\n",
        "# and the next time we call it, it will not know that g = 20 now"
      ],
      "metadata": {
        "id": "StvmFBb3w0KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what about working with conditionals?\n",
        "\n",
        "def relu(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 0\n",
        "try:\n",
        "    relu_jit = jax.jit(relu)(10)\n",
        "    result = relu_jit(-1)\n",
        "    print(result)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", str(e))\n",
        "\n",
        "# since the function's output depends on the input\n",
        "# its impossible to compile it in the way JAX wants it to behave"
      ],
      "metadata": {
        "id": "NvC5PVAgw3ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can get around this in a couple ways\n",
        "# one is with static arguments\n",
        "# this tells the compiler that the specified argument is allowed to change\n",
        "# and will recompile the function anytime it gets a new input\n",
        "# so this is best used when you don't expect a lot of different values\n",
        "# we also use jax.lax.cond, a control flow primitive that works with jit compilation\n",
        "\n",
        "# also note that we can use the @ decorator to jit a function rather than explicitly passing it\n",
        "\n",
        "@partial(jax.jit, static_argnames=['threshold'])\n",
        "def conditional_sum(x, y, threshold):\n",
        "    return jax.lax.cond(\n",
        "        x > threshold,\n",
        "        lambda: x + y,\n",
        "        lambda: x - y\n",
        "    )\n",
        "\n",
        "x_val = jnp.array(10.0)\n",
        "y_val = jnp.array(5.0)\n",
        "threshold_val = 7.0\n",
        "\n",
        "result = conditional_sum(x_val, y_val, threshold_val)\n",
        "print(result)\n",
        "\n",
        "x_val2 = jnp.array(6.0)\n",
        "y_val2 = jnp.array(3.0)\n",
        "\n",
        "result2 = conditional_sum(x_val2, y_val2, threshold_val)\n",
        "print(result2)\n",
        "\n",
        "threshold_val2 = 8.0\n",
        "\n",
        "result3 = conditional_sum(x_val, y_val, threshold_val2) # triggers recompilation\n",
        "print(result3)"
      ],
      "metadata": {
        "id": "0Pe37uKgw7jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another performance transformation jax provides is vmap\n",
        "# vmap automatically vectorizes a given function\n",
        "# by defining a batch dimension to work across\n",
        "# it basically does a [f(i) for i in x]\n",
        "\n",
        "# let's look at a basic example\n",
        "def add_and_square(x, y):\n",
        "    return (x + y) ** 2\n",
        "\n",
        "vectorized_add_and_square = jax.vmap(add_and_square)\n",
        "\n",
        "x = jnp.array([1, 2, 3])\n",
        "y = jnp.array([4, 5, 6])\n",
        "\n",
        "x1 = jnp.arange(10)\n",
        "y1 = jnp.arange(10)\n",
        "\n",
        "print(\"original jaxpr:\")\n",
        "print(jax.make_jaxpr(add_and_square)(x[0], y[0]))\n",
        "print(add_and_square(x[0],y[0]).shape)\n",
        "\n",
        "print(\"\\nvmapped jaxpr:\")\n",
        "print(jax.make_jaxpr(vectorized_add_and_square)(x, y))\n",
        "print(vectorized_add_and_square(x,y).shape)\n",
        "print(jax.make_jaxpr(vectorized_add_and_square)(x1, y1))\n",
        "print(vectorized_add_and_square(x1,y1).shape)\n",
        "\n",
        "# notice what happens in the jaxpr that's created\n",
        "# vmap figures out the batch dimension to vectorize on\n",
        "# we can specify this ourselves if needed with in_axes, out_axes\n",
        "# but we won't go over that\n",
        "# jit/vmap/grad are all functions\n",
        "# and all play well with each other\n",
        "# you can vmap a jitted function, etc"
      ],
      "metadata": {
        "id": "oh_OU6RIHNK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with jit and vmap, we are now ready to handle the tiny shakespeare dataset efficiently\n",
        "# as you saw earlier, we should avoid jit compiling functions that don't take in jax arrays\n",
        "# to turn text into a jnp array that will play well with jax\n",
        "# we can just define a preprocess function\n",
        "# make sure to explicitly pass in dtype so that we get integer values for each index\n",
        "\n",
        "\"\"\"\n",
        "def preprocess_text(text: str) -> jax.Array:\n",
        "    return\n",
        "\"\"\"\n",
        "\n",
        "def preprocess_text(text: str):\n",
        "    return jnp.array([ord(c) for c in text], dtype=jnp.int32)"
      ],
      "metadata": {
        "id": "dgBMNDcNAW_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can start by jit compiling the char_to_idx and idx_to_char functions\n",
        "# this is pretty much as simple as adding the jax.jit decorator\n",
        "\n",
        "\"\"\"\n",
        "def _char_to_idx(char: jax.Array):\n",
        "    return jnp.where((32 <= char) & (char <= 126), char - 32, 0)\n",
        "\n",
        "def _idx_to_char(index: jax.Array):\n",
        "    return jnp.where((0 <= index) & (index <= 94), index + 32, ord(' '))\n",
        "\"\"\"\n",
        "\n",
        "@jax.jit\n",
        "def _char_to_idx(char: jax.Array):\n",
        "    return jnp.where((32 <= char) & (char <= 126), char - 32, 0)\n",
        "\n",
        "@jax.jit\n",
        "def _idx_to_char(index: jax.Array):\n",
        "    return jnp.where((0 <= index) & (index <= 94), index + 32, ord(' '))\n"
      ],
      "metadata": {
        "id": "j8OcQY-eBcYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, we can use vmap to wrap those idx/char conversion functions\n",
        "# we should jit compile these functions as well\n",
        "\n",
        "\"\"\"\n",
        "def _tokenizer_encode_jax(char_array: jax.Array):\n",
        "    return # vmap\n",
        "\n",
        "def _tokenizer_decode_jax(ids: jax.Array):\n",
        "    return # vmap\n",
        "\"\"\"\n",
        "\n",
        "@jax.jit\n",
        "def _tokenizer_encode_jax(char_array: jax.Array):\n",
        "    return jax.vmap(_char_to_idx)(char_array)\n",
        "\n",
        "@jax.jit\n",
        "def _tokenizer_decode_jax(ids: jax.Array):\n",
        "    return jax.vmap(_idx_to_char)(ids)"
      ],
      "metadata": {
        "id": "74Who61RBP-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, we can write our final encode/decode functions\n",
        "\n",
        "def jit_encode(char_array):\n",
        "    return _tokenizer_encode_jax(char_array)\n",
        "\n",
        "def jit_decode(ids):\n",
        "    char_array = _tokenizer_decode_jax(ids)\n",
        "    return ''.join([chr(int(c)) for c in char_array])\n"
      ],
      "metadata": {
        "id": "XZW2zwq_BsCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jit_tokenizer = Tokenizer(encode=jit_encode, decode=jit_decode)"
      ],
      "metadata": {
        "id": "8YIIA0f0B_t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_array = preprocess_text(train_text)\n",
        "train_tokens = jit_tokenizer.encode(train_array)\n",
        "train_text[:20], train_array[:20], train_tokens[:20], jit_tokenizer.decode(train_tokens[:20])"
      ],
      "metadata": {
        "id": "l3PwXoYHCfJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### vmap intro (batched forward)"
      ],
      "metadata": {
        "id": "Eh1tM7P4wXA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jit_tokenizer.decode(jit_tokenizer.encode(preprocess_text('hello')))"
      ],
      "metadata": {
        "id": "ih42bw59pK9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another optimization to make sure we can process this dataset in reasonable time\n",
        "# is using batches of training data at a time\n",
        "# we'll grab some batch_size amount of sequences at once\n",
        "# and process them in parallel with jax's vmap\n",
        "# as a reminder, vmap makes it fast/easy to run\n",
        "# [f(i) for i in x]\n",
        "# across some batch dimension (like batch_size!)\n",
        "\n",
        "# we'll create a fake batch here to illustrate the concept\n",
        "\n",
        "tokens_1 = jit_tokenizer.encode(preprocess_text('hello'))  # First sequence\n",
        "tokens_2 = jit_tokenizer.encode(preprocess_text('world'))  # Second sequence\n",
        "tokens_3 = jit_tokenizer.encode(preprocess_text('test!'))   # Third sequence\n",
        "\n",
        "# Stack the sequences into a single batch with jnp.stack\n",
        "tokens_batch = jnp.stack([tokens_1, tokens_2, tokens_3])\n",
        "print(tokens_batch.shape)\n",
        "\n",
        "# we'll write a batch forward for our model\n",
        "# use jax.vmap and lambda x: to forward the specified model over the input batch\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def batch_forward(params: Dict, model_config: Dict, model_name: str, input_batch: jnp.ndarray) -> jnp.ndarray:\n",
        "    return ...\n",
        "\n",
        "logits = batch_forward(bigram_params, bigram_config, 'bigram_model', tokens_batch)\n",
        "logits.shape\n",
        "\n",
        "\"\"\"\n",
        "def batch_forward(params: Dict, model_config: Dict, model_name: str, input_batch: jnp.ndarray) -> jnp.ndarray:\n",
        "    return jax.vmap(lambda x: MODEL_DICT[model_name](params, model_config, x))(input_batch)\n",
        "\n",
        "logits = batch_forward(bigram_params, bigram_config, 'bigram_model', tokens_batch)\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "tzLsan5f9Tkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can evaluate our loss over these batches\n",
        "# generate a batch_size number of integer start indices\n",
        "# use those and jax.lax.dynamic_slice to get sequences\n",
        "# return the input/output batches\n",
        "# remember that [1, 2, 3, 4, 5]\n",
        "# would have input batch [1, 2, 3, 4]\n",
        "# and target batch [2, 3, 4, 5]\n",
        "\n",
        "\"\"\"\n",
        "def create_random_batches(tokens: jax.Array, batch_size: int, seq_len: int, key) -> Tuple[jax.Array, jax.Array]:\n",
        "    total_tokens = ...\n",
        "    max_start_idx = ...\n",
        "    start_indices = jax.random.randint(key, shape=..., minval = ..., maxval = max_start_idx)\n",
        "\n",
        "    def get_sequence(start_idx):\n",
        "        return jax.lax.dynamic_slice(...)\n",
        "\n",
        "    sequences = ... # use vmap\n",
        "    input_batches, target_batches = ... # remember targets should be offset by 1 from input but same size\n",
        "    return input_batches, target_batches\n",
        "\n",
        "\n",
        "input_batches, target_batches = create_random_batches(tokens=train_tokens, batch_size=4, seq_len=8, key=jax.random.PRNGKey(0))\n",
        "logit_batch = batch_forward(bigram_params, bigram_config, 'bigram_model', input_batches)\n",
        "cross_entropy_loss(logit_batch, target_batches)\n",
        "\"\"\"\n",
        "\n",
        "def create_random_batches(tokens: jax.Array, batch_size: int, seq_len: int, key):\n",
        "    max_start_idx = tokens.shape[0] - seq_len - 1\n",
        "    start_indices = jax.random.randint(\n",
        "        key,\n",
        "        shape=(batch_size,),\n",
        "        minval=0,\n",
        "        maxval=max_start_idx\n",
        "    )\n",
        "\n",
        "    def get_sequence(start_idx):\n",
        "        return jax.lax.dynamic_slice(tokens, (start_idx,), (seq_len + 1,))\n",
        "\n",
        "    sequences = jax.vmap(get_sequence)(start_indices)\n",
        "\n",
        "    input_batches, target_batches = sequences[:, :-1], sequences[:, 1:]\n",
        "\n",
        "    return input_batches, target_batches\n",
        "\n",
        "\n",
        "\n",
        "input_batches, target_batches = create_random_batches(tokens=train_tokens, batch_size=4, seq_len=8, key=jax.random.PRNGKey(0))\n",
        "logit_batch = batch_forward(bigram_params, bigram_config, 'bigram_model', input_batches)\n",
        "cross_entropy_loss(logit_batch, target_batches)"
      ],
      "metadata": {
        "id": "YU59VqI_82Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, we can also parallelize our model over an input\n",
        "# here we'll use jax vmap\n",
        "# as a reminder, it works similarly to [f(i) for i in x] over some dimension\n",
        "\n",
        "# as a start\n",
        "# rather than predicting separately for each position in the seq_len\n",
        "# we can do our predictions in parallel across the seq_len dimension\n",
        "# so instead of outputting (batch_size, 1, vocab_size)\n",
        "# we output (batch_size, seq_len, vocab_size)\n",
        "# we'll just have to make sure to select the last logit for generation\n",
        "\n",
        "\"\"\"\n",
        "def parallel_bigram(model_params: Dict, model_config: Dict, tokens: jax.Array) -> jax.Array:\n",
        "    logits = jax.vmap(lambda x: ...)(tokens)\n",
        "    return logits\n",
        "\n",
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'parallel_bigram': parallel_bigram\n",
        "}\n",
        "\n",
        "logits = parallel_bigram(bigram_params, bigram_config, input_batches)\n",
        "logits.shape\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def parallel_bigram(model_params: Dict, model_config: Dict, tokens: jax.Array) -> jax.Array:\n",
        "    logits = jax.vmap(lambda x: model_params['token_table'][x])(tokens)\n",
        "    return logits\n",
        "\n",
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'parallel_bigram': parallel_bigram\n",
        "}\n",
        "\n",
        "logits = parallel_bigram(bigram_params, bigram_config, input_batches)\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "8RkeOKi56-mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training"
      ],
      "metadata": {
        "id": "NlKUuTQOQVVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we now have everything we need to train our model!\n",
        "# typically in training deep learning models\n",
        "# we work through our data in epochs\n",
        "# traditionally, this is a full pass through of our dataset\n",
        "# but it can also just mean a fixed amount of batches\n",
        "# in each batch, we compute the loss over the batch, and do an update step\n",
        "# finally, we return the trained parameters\n",
        "\n",
        "# let's see what that looks like for our language model\n",
        "# we'll start by creating the TrainConfig struct\n",
        "# we'll define the number of epochs\n",
        "# the number of batches per epoch\n",
        "# the amount of sequences in each batch\n",
        "# the sequence length of each batch\n",
        "# and a seed for our key\n",
        "\n",
        "class TrainConfig(NamedTuple):\n",
        "    num_epochs: int\n",
        "    batches_per_epoch: int\n",
        "    batch_size: int\n",
        "    batch_seq_len: int\n",
        "    seed: int"
      ],
      "metadata": {
        "id": "_xwnqGi4G3EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can write one training step\n",
        "# we'll use jax.jit to compile this so training goes faster\n",
        "# use the batch_forward we defined above to efficiently compute logits for an input batch\n",
        "# define a batch_loss_function to compute the loss against a target batch (nested inside, so it also gets compiled)\n",
        "# use create random batches from before as well\n",
        "\n",
        "# our training step should return the new parameters, optimizer state, and the loss for the step\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@partial(jax.jit, static_argnames=[])\n",
        "def train_step(params: Dict, model_config: ModelConfig, model_name: str, tokens: jax.Array, opt_state: Dict, opt_config: OptConfig, train_config: TrainConfig, key):\n",
        "    def batch_loss_fn(params, input_batch, target_batch):\n",
        "        logits = ...\n",
        "        batch_loss = ...\n",
        "        return ...\n",
        "\n",
        "    batch_size = ...\n",
        "    seq_len = ...\n",
        "    input_batch, target_batch = ...\n",
        "    loss, grads = ...\n",
        "    new_params, new_opt_state = ...\n",
        "    return new_params, new_opt_state, loss\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@partial(jax.jit, static_argnames=['model_config', 'model_name', 'opt_config', 'train_config', ])\n",
        "def train_step(params: Dict, model_config: ModelConfig, model_name: str, tokens: jax.Array, opt_state: Dict, opt_config: OptConfig, train_config: TrainConfig, key):\n",
        "    def batch_loss_fn(params, input_batch, target_batch):\n",
        "        logits = batch_forward(params, model_config, model_name, input_batch)\n",
        "        batch_loss = cross_entropy_loss(logits, target_batch)\n",
        "        return batch_loss\n",
        "\n",
        "    batch_size = train_config.batch_size\n",
        "    seq_len = train_config.batch_seq_len\n",
        "    input_batch, target_batch = create_random_batches(tokens, batch_size, seq_len, key)\n",
        "    loss, grads = jax.value_and_grad(batch_loss_fn)(params, input_batch, target_batch)\n",
        "    new_params, new_opt_state = opt_config.opt_update(params, grads, opt_state, opt_config)\n",
        "    return new_params, new_opt_state, loss"
      ],
      "metadata": {
        "id": "c7gO0X_KZCug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now that we have our training step defined\n",
        "# we can write our training loop\n",
        "# this is also the step where we can use an accelerator\n",
        "# we'll use jax.device_put() to put our parameters, tokens, and optimizer state onto the device\n",
        "# and we can get our device from jax.devices()\n",
        "\n",
        "# for each epoch\n",
        "# we call the train step for each of the batches in the epoch\n",
        "# add up the losses by epoch\n",
        "# and then we'll average the loss over each epoch\n",
        "# finally we will return the trained parameters"
      ],
      "metadata": {
        "id": "w04xpBA1hWmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def train(params: Dict, model_config: ModelConfig, model_name: str, tokens: jnp.ndarray, opt_config: OptConfig, train_config: TrainConfig, key=jax.random.PRNGKey(0)):\n",
        "    if model_name not in MODEL_DICT:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "    device =  # jax.devices() returns list of at least size 1\n",
        "    print(f\"using device: {device}\")\n",
        "\n",
        "    # initialize optimizer state\n",
        "\n",
        "    init_opt_state = ...\n",
        "\n",
        "    # use seed from train config to define initial key\n",
        "    key = ...\n",
        "\n",
        "    # move data to device\n",
        "    params = ...\n",
        "    tokens = ...\n",
        "\n",
        "    opt_state = ...\n",
        "\n",
        "    print('training ...')\n",
        "\n",
        "    # you may need to change these\n",
        "\n",
        "    for each epoch:\n",
        "        epoch_loss = 0.0\n",
        "        for each batch:\n",
        "            key, subkey = jax.random.split(key, 2)\n",
        "            new_params, opt_state, loss = train_step()\n",
        "            # add to epoch loss\n",
        "            params = new_params\n",
        "        avg_loss = ...\n",
        "        print(f\"epoch {}/{}, average Loss: {avg_loss}\")\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "train_config = TrainConfig(num_epochs=200, batches_per_epoch=128, batch_size=8, batch_seq_len=16, seed=0)\n",
        "opt_config = OptConfig(lr=1e-2, opt_init=init_sgd_state, opt_update=sgd_update)\n",
        "\n",
        "trained_bigram_params = train(bigram_params, bigram_config, 'bigram_model', train_tokens, sgd_config, train_config)\n",
        "\"\"\"\n",
        "\n",
        "def train(params: Dict, model_config: ModelConfig, model_name: str, tokens: jnp.ndarray, opt_config: OptConfig, train_config: TrainConfig, key=jax.random.PRNGKey(0)):\n",
        "    if model_name not in MODEL_DICT:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "    device = jax.devices()[0]\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    key = jax.random.PRNGKey(train_config.seed)\n",
        "    init_opt_state = opt_config.opt_init(params, opt_config)\n",
        "\n",
        "    # Move data to device\n",
        "    params = jax.device_put(params, device)\n",
        "    tokens = jax.device_put(tokens, device)\n",
        "    opt_state = jax.device_put(init_opt_state, device)\n",
        "\n",
        "\n",
        "    print('training ...')\n",
        "    for epoch in range(train_config.num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for _ in range(train_config.batches_per_epoch):\n",
        "            key, subkey = jax.random.split(key, 2)\n",
        "            new_params, opt_state, loss = train_step(\n",
        "                params, model_config, model_name, tokens, opt_state, opt_config, train_config, key=subkey\n",
        "            )\n",
        "            epoch_loss += loss\n",
        "\n",
        "            params = new_params\n",
        "        avg_loss = epoch_loss / train_config.batches_per_epoch\n",
        "        print(f\"Epoch {epoch + 1}/{train_config.num_epochs}, Average Loss: {avg_loss}\")\n",
        "\n",
        "    return params\n",
        "\n",
        "train_config = TrainConfig(num_epochs=200, batches_per_epoch=128, batch_size=8, batch_seq_len=16, seed=0)\n",
        "opt_config = OptConfig(lr=1e-2, opt_init=init_sgd_state, opt_update=sgd_update)\n",
        "\n",
        "trained_bigram_params = train(bigram_params, bigram_config, 'bigram_model', train_tokens, sgd_config, train_config)"
      ],
      "metadata": {
        "id": "_F9hTHaoY_hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets compare with our initial generation!\n",
        "\n",
        "\"\"\"\n",
        "def compare_params(prompt: str, model_config: ModelConfig, model_name: str, params1: Dict, params2: Dict, max_new=10, temp=0.8, key=jax.random.PRNGKey(0)):\n",
        "\n",
        "    tokenized_prompt = ... # make sure to cast to integer\n",
        "    generated_tokens = # generate with params1\n",
        "    generated_tokens2 = # params2\n",
        "\n",
        "    generated_text = ... # decode\n",
        "    generated_text2 = ...\n",
        "\n",
        "    print(jit_tokenizer.decode(tokenized_prompt))\n",
        "    print(prompt + generated_text)\n",
        "    print(prompt + generated_text2)\n",
        "\n",
        "    return generated_text, generated_text2\n",
        "\n",
        "compare_params('shakespeare', bigram_config, 'bigram_model', bigram_params, trained_bigram_params)\n",
        "\"\"\"\n",
        "\n",
        "def compare_params(prompt: str, model_config: ModelConfig, model_name: str, params1: Dict, params2: Dict):\n",
        "\n",
        "    tokenized_prompt = jnp.array(jit_tokenizer.encode(preprocess_text(prompt)), dtype=jnp.int32)\n",
        "    generated_tokens = generate(params1, model_config, model_name, tokens=tokenized_prompt, max_new=10, temp=1, key=jax.random.PRNGKey(0))\n",
        "    generated_tokens2 = generate(params2, model_config, model_name, tokens=tokenized_prompt, max_new=10, temp=1, key=jax.random.PRNGKey(0))\n",
        "\n",
        "    generated_text = jit_tokenizer.decode(generated_tokens)\n",
        "    generated_text2 = jit_tokenizer.decode(generated_tokens2)\n",
        "    print(jit_tokenizer.decode(tokenized_prompt))\n",
        "    print(prompt + generated_text)\n",
        "    print(prompt + generated_text2)\n",
        "\n",
        "prompt = jit_tokenizer.decode(train_tokens[:21])\n",
        "compare_params(prompt, bigram_config, 'bigram_model', bigram_params, trained_bigram_params)"
      ],
      "metadata": {
        "id": "_K85jZkXHAUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### token embeddings"
      ],
      "metadata": {
        "id": "m-ui2K4cwafY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# so it seems like bigram models kind of suck\n",
        "# our loss is going down, but the generations are horrendous\n",
        "\n",
        "# by themselves, our characters dont really mean anything\n",
        "# for example, if e is 5 and j is 10\n",
        "# e * 2 isn't really something that should make sense to do\n",
        "# but we've implied that it is the case\n",
        "\n",
        "# also, one integer is not a lot of information\n",
        "# instead, we might want to represent each character as a vector\n",
        "# we can do this by having our model learn embedding vectors\n",
        "# the size of these vectors is the embedding dimension\n",
        "\n",
        "# for example, a might start out as [?,?]\n",
        "# and up as [0,1]\n",
        "# where the first dimension is about being a consonant\n",
        "# and the second is about if you can say the letter without moving your lips\n",
        "# obviously thats a toy example\n",
        "# but the idea is that the more dimensions we add\n",
        "# the more information the model can encode about a token as it learns\n",
        "\n",
        "# we'll need two matrices: one to turn the initial vocab into embedding vectors, and vice versa\n",
        "# we call these token_embedding and output_projection\n",
        "# and we'll need to update our model config struct to include an embedding dimension\n",
        "# we'll also introduce a scaling factor\n",
        "# to initialize our model with weights drawn from N(0,sigma) instead of N(0,1)\n",
        "\n",
        "class ModelConfig(NamedTuple):\n",
        "    vocab_size: int\n",
        "    embedding_dim: int\n",
        "\n",
        "\"\"\"\n",
        "def init_bigram_embed_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    k1, k2 = jax.random.split(key)\n",
        "    return {\n",
        "        'token_embedding': ,\n",
        "        'output_projection': ,\n",
        "    }\n",
        "\n",
        "\n",
        "# remember: the lookup table means the ith row vector is the ith token in our vocab\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "b_embed_config = ModelConfig(vocab_size=95, embedding_dim = 5)\n",
        "b_embed_params = init_bigram_embed_params(key, b_embed_config)\n",
        "\n",
        "idx = tokenizer.encode('a')\n",
        "input_embedding = b_embed_params['token_embedding'][idx]\n",
        "print(input_embedding)\n",
        "\n",
        "output_projection = b_embed_params['output_projection']\n",
        "logits = jnp.dot(input_embedding, output_projection)\n",
        "print(logits.shape)\n",
        "\"\"\"\n",
        "\n",
        "def init_bigram_embed_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    k1, k2 = jax.random.split(key)\n",
        "    return {\n",
        "        'token_embedding': jax.random.normal(k1, (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(k2, (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor\n",
        "    }\n",
        "\n",
        "# remember: the lookup table means the ith row vector is the ith token in our vocab\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "b_embed_config = ModelConfig(vocab_size=95, embedding_dim = 5)\n",
        "b_embed_params = init_bigram_embed_params(key, b_embed_config)\n",
        "\n",
        "idx = tokenizer.encode('a')\n",
        "input_embedding = b_embed_params['token_embedding'][idx]\n",
        "print(input_embedding)\n",
        "\n",
        "output_projection = b_embed_params['output_projection']\n",
        "logits = jnp.dot(input_embedding, output_projection)\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "id": "su9pPJhhG06t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to forward pass through this model\n",
        "# you need to get the corresponding token embeddings for each token in (seq_len) from your token embedding table\n",
        "# then use jnp.dot to project the embedded tokens back into the vocab size dimension with the output projection matrix\n",
        "\n",
        "\"\"\"\n",
        "def bigram_embed(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    token_embedding = model_params['token_embedding']\n",
        "    output_projection = model_params['output_projection']\n",
        "\n",
        "    embedded = ... # (seq_len,) -> (seq_len, embedding_dim)\n",
        "\n",
        "    logits = ... # (seq_len, embedding_dim) @ (embedding_dim, vocab_size) -> (seq_len, vocab_size)\n",
        "\n",
        "    return logits\n",
        "\n",
        "bigram_embed(b_embed_params, b_embed_config, tokenizer.encode('abc')).shape\n",
        "\n",
        "# now we just update model dict and train again\n",
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed\n",
        "}\n",
        "\n",
        "train_config = TrainConfig(num_epochs=50, batches_per_epoch=64, batch_size=16, batch_seq_len=32, seed=0)\n",
        "trained_b_embed_params = train(b_embed_params, b_embed_config, 'bigram_embed', train_tokens, sgd_config, train_config)\n",
        "compare_params('shakespeare', b_embed_config, 'bigram_embed', b_embed_params, trained_b_embed_params)\n",
        "\"\"\"\n",
        "\n",
        "def bigram_embed(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    token_embedding = model_params['token_embedding']\n",
        "    output_projection = model_params['output_projection']\n",
        "\n",
        "    embedded = token_embedding[tokens] # (seq_len,) -> (seq_len, embedding_dim)\n",
        "\n",
        "    logits = jnp.dot(embedded, output_projection) # (seq_len, embedding_dim) @ (embedding_dim, vocab_size) -> (seq_len, vocab_size)\n",
        "\n",
        "    return logits\n",
        "\n",
        "bigram_embed(b_embed_params, b_embed_config, tokenizer.encode('abc')).shape\n",
        "\n",
        "# now we just update model dict and train again\n",
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed\n",
        "}\n",
        "\n",
        "train_config = TrainConfig(num_epochs=50, batches_per_epoch=64, batch_size=16, batch_seq_len=32, seed=0)\n",
        "trained_b_embed_params = train(b_embed_params, b_embed_config, 'bigram_embed', train_tokens, sgd_config, train_config)\n",
        "prompt = jit_tokenizer.decode(train_tokens[:21])\n",
        "compare_params(prompt, b_embed_config, 'bigram_embed', b_embed_params, trained_b_embed_params)"
      ],
      "metadata": {
        "id": "d5w6h33hMF12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### new optimization strategy"
      ],
      "metadata": {
        "id": "C-xOlHFKNtaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you'll notice the loss went down but not by a lot\n",
        "# and the sequence generated was basically the same\n",
        "# another possibility is that our optimization algorithm needs work\n",
        "# stochastic gradient descent is nice, but it depends only on the current gradient of the hill\n",
        "# you can imagine the parameters walking down the loss surface, specifically taking a step based on the slope\n",
        "# as you can see, this is kind of slow\n",
        "# and it would easily get stuck in a small valley\n",
        "# one way to speed up learning and roll through small valleys is with momentum\n",
        "# rather than having a person taking steps on the loss surface\n",
        "# picture a ball that rolls into the loss surface\n",
        "# the ball carries momentum from its previous actions into the next action it takes\n",
        "# causing it to quickly go over valleys that are not the true bottom of the loss surface\n",
        "# this will hopefully speed up the training process\n",
        "\n",
        "# updated_param = param - (learning_rate * velocity)\n",
        "\n",
        "# SGD momentum update rule\n",
        "# velocity = (old_velocity * momentum) + gradient"
      ],
      "metadata": {
        "id": "6hKTenueNvba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# while SGD with momentum works well in some problems\n",
        "# it was noticed empirically that different parameters made a big impact on gradients\n",
        "# and the fixed, global learning rate meant different parameters couldn't update independently\n",
        "# RMSProp is an optimizer that looks at the moving average of the squared gradient\n",
        "# and updates parameters using that value\n",
        "\n",
        "# squared_gradient_average = 0\n",
        "# decay_rate = 0.9  # typically 0.9\n",
        "# epsilon = 1e-8  # avoids div by 0\n",
        "\n",
        "# # for each parameter update\n",
        "# squared_gradient_average = (decay_rate * squared_gradient_average) + ((1 - decay_rate) * gradient ** 2)\n",
        "\n",
        "# # RMSprop update rule\n",
        "# adjusted_gradient = gradient / (sqrt(squared_gradient_average) + epsilon)\n",
        "# updated_param = param - (learning_rate * adjusted_gradient)"
      ],
      "metadata": {
        "id": "AZ5hT4F7Nr_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the adam optimizer combines ideas from rmsprop and sgd with momntum\n",
        "# similarly to how RMSProp takes the moving average of the squared gradient to update\n",
        "# Adam also calculates a moving average of the gradient to adjust parameters\n",
        "# at the start, it initializes averages to 0\n",
        "# but this effectively implies every previous measurement was a 0\n",
        "# leading to very small estimations\n",
        "# so we introduce a bias correction mechanism that depends on the time step\n",
        "\n",
        "\"\"\"\n",
        "# initialize\n",
        "gradient_mean = 0  # average of gradient\n",
        "gradient_sq_mean = 0  # average of squared gradient\n",
        "beta_1 = 0.9  # decay rate for first moment\n",
        "beta_2 = 0.999  # decay rate for second moment\n",
        "epsilon = 1e-8  # to avoid division by zero\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.01\n",
        "t = 0  # timestep\n",
        "\n",
        "# update for each parameter\n",
        "t += 1\n",
        "\n",
        "# update biased first moment estimate\n",
        "gradient_mean = (beta_1 * gradient_mean) + ((1 - beta_1) * gradient)\n",
        "\n",
        "# update biased second raw moment estimate\n",
        "gradient_sq_mean = (beta_2 * gradient_sq_mean) + ((1 - beta_2) * gradient ** 2)\n",
        "\n",
        "# compute bias-corrected first moment estimate\n",
        "gradient_mean_corrected = gradient_mean / (1 - beta_1 ** t)\n",
        "\n",
        "# compute bias-corrected second raw moment estimate\n",
        "gradient_sq_mean_corrected = gradient_sq_mean / (1 - beta_2 ** t)\n",
        "\n",
        "# compute final AdamW parameter update\n",
        "adjusted_gradient = gradient_mean_corrected / (sqrt(gradient_sq_mean_corrected) + epsilon)\n",
        "param_update = learning_rate * (adjusted_gradient + weight_decay * param) # weight decay adjustment\n",
        "\n",
        "# apply update\n",
        "updated_param = param - param_update\n",
        "\"\"\"\n",
        "\n",
        "class OptConfig(NamedTuple):\n",
        "    lr: float\n",
        "    beta1: float\n",
        "    beta2: float\n",
        "    weight_decay: float\n",
        "    eps: float\n",
        "    opt_init: Callable\n",
        "    opt_update: Callable\n",
        "\n",
        "class OptimizerState(NamedTuple):\n",
        "    step: int\n",
        "    gradient_mean: float\n",
        "    gradient_squared_mean: float"
      ],
      "metadata": {
        "id": "andx9TDsZRQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def init_adam_state(params: Dict, opt_config: OptConfig) -> OptimizerState:\n",
        "    gradient_mean = ...\n",
        "    gradient_squared_mean = ...\n",
        "    return OptimizerState(\n",
        "        gradient_mean=gradient_mean,\n",
        "        gradient_squared_mean=gradient_squared_mean,\n",
        "        step=0\n",
        "    )\n",
        "\n",
        "def adamw_update(params: Dict, grads: Dict, opt_state: OptimizerState, opt_config: OptConfig) -> Tuple[Dict, Dict]:\n",
        "\n",
        "    current_step = opt_state.step + 1\n",
        "\n",
        "    gradient_mean_biased = ...\n",
        "\n",
        "    gradient_squared_mean_biased = ...\n",
        "\n",
        "    gradient_mean_corrected = ...\n",
        "\n",
        "    gradient_squared_mean_corrected = ...\n",
        "\n",
        "    param_updates = ...\n",
        "\n",
        "    updated_params = ...\n",
        "\n",
        "    updated_opt_state = OptimizerState(\n",
        "        gradient_mean=gradient_mean_biased,\n",
        "        gradient_squared_mean=gradient_squared_mean_biased,\n",
        "        step=current_step\n",
        "    )\n",
        "\n",
        "    return updated_params, updated_opt_state\n",
        "\n",
        "adamw_config = OptConfig(lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01, opt_init=init_adam_state, opt_update=adamw_update)\n",
        "\"\"\"\n",
        "\n",
        "def init_adam_state(params: Dict, opt_config: OptConfig) -> OptimizerState:\n",
        "    gradient_mean = jax.tree_map(lambda p: jnp.zeros_like(p), params)\n",
        "    gradient_squared_mean = jax.tree_map(lambda p: jnp.zeros_like(p), params)\n",
        "    return OptimizerState(\n",
        "        gradient_mean=gradient_mean,\n",
        "        gradient_squared_mean=gradient_squared_mean,\n",
        "        step=0\n",
        "    )\n",
        "\n",
        "def adamw_update(params: Dict, grads: Dict, opt_state: OptimizerState, opt_config: OptConfig) -> Tuple[Dict, Dict]:\n",
        "\n",
        "    current_step = opt_state.step + 1\n",
        "\n",
        "    gradient_mean_biased = jax.tree.map(\n",
        "        lambda prev_mean, gradient: opt_config.beta1 * prev_mean + (1 - opt_config.beta1) * gradient,\n",
        "        opt_state.gradient_mean,\n",
        "        grads\n",
        "    )\n",
        "\n",
        "    gradient_squared_mean_biased = jax.tree.map(\n",
        "        lambda prev_squared_mean, gradient: opt_config.beta2 * prev_squared_mean + (1 - opt_config.beta2) * gradient**2,\n",
        "        opt_state.gradient_squared_mean,\n",
        "        grads\n",
        "    )\n",
        "\n",
        "    gradient_mean_corrected = jax.tree.map(\n",
        "        lambda mean: mean / (1 - opt_config.beta1**current_step),\n",
        "        gradient_mean_biased\n",
        "    )\n",
        "\n",
        "    gradient_squared_mean_corrected = jax.tree.map(\n",
        "        lambda squared_mean: squared_mean / (1 - opt_config.beta2**current_step),\n",
        "        gradient_squared_mean_biased\n",
        "    )\n",
        "\n",
        "    param_updates = jax.tree.map(\n",
        "        lambda mean, squared_mean, param: -opt_config.lr * (mean / (jnp.sqrt(squared_mean) + opt_config.eps) + opt_config.weight_decay * param),\n",
        "        gradient_mean_corrected,\n",
        "        gradient_squared_mean_corrected,\n",
        "        params\n",
        "    )\n",
        "\n",
        "    updated_params = jax.tree.map(\n",
        "        lambda param, update: param + update, params, param_updates)\n",
        "\n",
        "    updated_opt_state = OptimizerState(\n",
        "        gradient_mean=gradient_mean_biased,\n",
        "        gradient_squared_mean=gradient_squared_mean_biased,\n",
        "        step=current_step\n",
        "    )\n",
        "\n",
        "    return updated_params, updated_opt_state\n",
        "\n",
        "adamw_config = OptConfig(lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01, opt_init=init_adam_state, opt_update=adamw_update)\n"
      ],
      "metadata": {
        "id": "T3KMm-TaZW2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we just update model dict and train again\n",
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed\n",
        "}\n",
        "\n",
        "train_config = TrainConfig(num_epochs=50, batches_per_epoch=32, batch_size=32, batch_seq_len=64, seed=0)\n",
        "adamw_config = OptConfig(lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01, opt_init=init_adam_state, opt_update=adamw_update)\n",
        "trained_b_embed_params = train(b_embed_params, b_embed_config, 'bigram_embed', train_tokens, adamw_config, train_config)\n",
        "prompt = jit_tokenizer.decode(train_tokens[:21])\n",
        "compare_params(prompt, b_embed_config, 'bigram_embed', b_embed_params, trained_b_embed_params)\n"
      ],
      "metadata": {
        "id": "KvN9WtgqOAPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### causal masking"
      ],
      "metadata": {
        "id": "nHLdfCujwdnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# while we saw some change in generation\n",
        "# its still kind of unintelligible\n",
        "# this is likely due to the fact that bigram models are weak\n",
        "# even with the embedding vector, we are still only using the previous token to output our prediction\n",
        "# naturally this is not enough"
      ],
      "metadata": {
        "id": "xXS40vbYOTql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so rather than just looking at the previous token\n",
        "# let's update our model to look at all previous tokens\n",
        "# remember that we're doing our predictions in parallel\n",
        "# so we need a way to make sure that we don't look ahead in time to future tokens\n",
        "# otherwise we would be letting the model peek at the right answer\n",
        "# how might we do this?\n",
        "\n",
        "# let's work with a small case, of sequence length 5\n",
        "# the first token can only pay attention to itself\n",
        "# the second token can look back by 1, and itself,\n",
        "# etc\n",
        "# 1 0 0 0 0\n",
        "# 1 1 0 0 0\n",
        "# 1 1 1 0 0\n",
        "# ...\n",
        "# this ends up turning into a matrix of (seq_len, seq_len) shape, with all 1s and 0s\n",
        "# the 1s follow a specific shape known as lower triangular\n",
        "# and jax has a built in function to create one of these matrices"
      ],
      "metadata": {
        "id": "lJUrrzvZOVWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we call this a 'causal mask' as it preserves causality,\n",
        "# it prevents info from previous tokens going back in time\n",
        "# use jnp tril and jnp ones to create the causal mask for the seq_len\n",
        "\n",
        "\"\"\"\n",
        "seq_len = 5\n",
        "causal_mask = ...\n",
        "causal_mask\n",
        "\"\"\"\n",
        "\n",
        "seq_len = 5\n",
        "causal_mask = jnp.tril(jnp.ones((seq_len,seq_len)))\n",
        "causal_mask"
      ],
      "metadata": {
        "id": "67uczackOW20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, we just need to divide each row by its sum\n",
        "# to get the equal weighting for each token in the sequence\n",
        "# you can use jnp sum for this, you may need some of the optional parameters like 'axis' and 'keepdims'\n",
        "\"\"\"\n",
        "\n",
        "causal_weights = ...\n",
        "causal_weights\n",
        "\"\"\"\n",
        "\n",
        "causal_weights = causal_mask / jnp.sum(causal_mask, axis=1, keepdims=True)\n",
        "causal_weights"
      ],
      "metadata": {
        "id": "_KaOv_02OYZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, averaging the embedding vectors with our causal weights is just a matrix multiplication!\n",
        "# and our final output sends these contexually weighted vectors back into the vocab_size dimension\n",
        "\n",
        "# lets update our basic bigram model to equally attend to each of the previous tokens\n",
        "\n",
        "\"\"\"\n",
        "def causal_embed(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array: # (seq_len,) -> (seq_len, vocab_size)\n",
        "\n",
        "    token_embedding = model_params['token_embedding']\n",
        "    output_projection = model_params['output_projection']\n",
        "\n",
        "    embedded = token_embedding[tokens] # (seq_len,) -> (seq_len, embedding_dim)\n",
        "\n",
        "    seq_len = tokens.shape[0]\n",
        "\n",
        "    causal_mask = ...\n",
        "\n",
        "    causal_weights = ...\n",
        "\n",
        "    context_vectors = ... # (seq_len, seq_len) @ (seq_len, embedding_dim) -> (seq_len, embedding_dim)\n",
        "\n",
        "    logits = ... # (seq_len, embedding_dim) @ (embedding_dim, vocab_size) -> (seq_len, vocab_size)\n",
        "\n",
        "    return logits\n",
        "\"\"\"\n",
        "\n",
        "def causal_embed(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array: # (seq_len,) -> (seq_len, vocab_size)\n",
        "\n",
        "    token_embedding = model_params['token_embedding']\n",
        "    output_projection = model_params['output_projection']\n",
        "\n",
        "    embedded = token_embedding[tokens] # (seq_len,) -> (seq_len, embedding_dim)\n",
        "\n",
        "    seq_len = tokens.shape[0]\n",
        "    causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "    causal_weights = causal_mask / jnp.sum(causal_mask, axis=1, keepdims=True)\n",
        "\n",
        "    context_vectors = jnp.dot(causal_weights, embedded) # (seq_len, seq_len) @ (seq_len, embedding_dim) -> (seq_len, embedding_dim)\n",
        "\n",
        "    logits = jnp.dot(context_vectors, output_projection) # (seq_len, embedding_dim) @ (embedding_dim, vocab_size) -> (seq_len, vocab_size)\n",
        "\n",
        "    return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "VvyzwKxjOdM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can just retrain after adding the params to the model dict\n",
        "# we won't change anything about the training configuration just yet\n",
        "\n",
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed,\n",
        "    'causal_embed': causal_embed\n",
        "}\n",
        "trained_c_embed_params = train(b_embed_params, b_embed_config, 'causal_embed', train_tokens, adamw_config, train_config)\n",
        "compare_params(prompt, b_embed_config, 'causal_embed', b_embed_params, trained_c_embed_params)\n"
      ],
      "metadata": {
        "id": "WPViiEgBOwAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### positional encoding"
      ],
      "metadata": {
        "id": "HT0ID4e8wezm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now what if we run two separate inputs\n",
        "# what do we expect to happen?\n",
        "\n",
        "\"\"\"\n",
        "shuffled_prompt = ... # use jax.random.permutation and the 0 key, as well as the jit_tokenizer functions and preprocess text from before\n",
        "\"\"\"\n",
        "\n",
        "shuffled_prompt = jit_tokenizer.decode(jax.random.permutation(jax.random.PRNGKey(0), jit_tokenizer.encode(preprocess_text(prompt))))\n",
        "print(prompt + '\\n' + shuffled_prompt)\n",
        "\n",
        "compare_params(prompt, b_embed_config, 'causal_embed', b_embed_params, trained_c_embed_params)\n",
        "compare_params(shuffled_prompt, b_embed_config, 'causal_embed', b_embed_params, trained_c_embed_params)\n",
        "\n",
        "# if you look at the part where the model starts generating\n",
        "# you can see we got the same output, even after shuffling\n",
        "\n",
        "# while we're now looking at all previous tokens\n",
        "# we haven't really given the model a way to understand position\n",
        "# we're simply averaging over the previous tokens equally"
      ],
      "metadata": {
        "id": "vignD5ZgOhHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarly to what we did with token embedding\n",
        "# we'll give the model a matrix it can use to learn about position\n",
        "# so we want the ith row vector to encode information about position i in a sequence\n",
        "# we cannot make a matrix of infinite size, so we'll need to limit the seq_len to some maximum\n",
        "# so we'll create a context_length parameter\n",
        "# and now our positional embedding matrix is of shape (context_len, embedding_dim)\n",
        "\n",
        "class ModelConfig(NamedTuple):\n",
        "    vocab_size: int\n",
        "    embedding_dim: int\n",
        "    context_len: int\n",
        "\n",
        "# define initialization function\n",
        "\n",
        "\"\"\"\n",
        "def init_causal_pos_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    k1, k2, k3 = jax.random.split(key, 3)\n",
        "    return {\n",
        "        'token_embedding': jax.random.normal(k1, (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': ,\n",
        "        'output_projection': jax.random.normal(k3, (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "def init_causal_pos_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    k1, k2, k3 = jax.random.split(key, 3)\n",
        "    return {\n",
        "        'token_embedding': jax.random.normal(k1, (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': jax.random.normal(k2, (model_config.context_len, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(k3, (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor\n",
        "    }"
      ],
      "metadata": {
        "id": "FCnBpPRJQTjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define forward pass\n",
        "# add the positional embeddings\n",
        "\n",
        "\"\"\"\n",
        "def causal_pos(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    token_embedding = model_params['token_embedding']\n",
        "    output_projection = model_params['output_projection']\n",
        "\n",
        "    token_embedded = token_embedding[tokens] # (seq_len,) -> (seq_len, embedding_dim)\n",
        "\n",
        "    # add positional embeddings\n",
        "    pos_embeds = ... # remember that we only need embeddings up to seq_len\n",
        "    embedded = token_embeds + pos_embeds\n",
        "\n",
        "    seq_len = tokens.shape[0]\n",
        "\n",
        "    causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "    causal_weights = causal_mask / jnp.sum(causal_mask, axis=1, keepdims=True)\n",
        "\n",
        "    context_vectors = jnp.dot(causal_weights, embedded) # (seq_len, seq_len) @ (seq_len, embedding_dim) -> (seq_len, embedding_dim)\n",
        "\n",
        "    logits = jnp.dot(context_vectors, output_projection) # (seq_len, embedding_dim) @ (embedding_dim, vocab_size) -> (seq_len, vocab_size)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "\n",
        "    return\n",
        "\"\"\"\n",
        "\n",
        "def causal_pos(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    token_embedding = model_params['token_embedding']\n",
        "    positional_embedding = model_params['positional_embedding']\n",
        "    output_projection = model_params['output_projection']\n",
        "\n",
        "    token_embeds = token_embedding[tokens]\n",
        "\n",
        "    seq_len = tokens.shape[0]\n",
        "    pos_embeds = positional_embedding[:seq_len]\n",
        "    embedded = token_embeds + pos_embeds\n",
        "\n",
        "    causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "    causal_weights = causal_mask / jnp.sum(causal_mask, axis=1, keepdims=True)\n",
        "\n",
        "    context_vectors = jnp.dot(causal_weights, embedded)\n",
        "\n",
        "    logits = jnp.dot(context_vectors, output_projection)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "KExFsWaqQf2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we just update model dict and train again\n",
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed,\n",
        "    'causal_embed': causal_embed,\n",
        "    'causal_pos': causal_pos\n",
        "}\n",
        "\n",
        "c_pos_config = ModelConfig(vocab_size = 95, embedding_dim = 4, context_len = 64)\n",
        "train_config = TrainConfig(num_epochs=50, batches_per_epoch=32, batch_size=32, batch_seq_len=64, seed=0)\n",
        "c_pos_params = init_causal_pos_params(key, c_pos_config)\n",
        "trained_c_pos_params = train(c_pos_params, c_pos_config, 'causal_pos', train_tokens, adamw_config, train_config)\n",
        "compare_params(prompt, c_pos_config, 'causal_pos', c_pos_params, trained_c_pos_params)\n",
        "compare_params(shuffled_prompt, c_pos_config, 'causal_pos', c_pos_params, trained_c_pos_params)"
      ],
      "metadata": {
        "id": "lncJ09HfQyz7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### attention"
      ],
      "metadata": {
        "id": "nCFoSLOEwf08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# although we now understand position\n",
        "# we're still prioritizing each character equally\n",
        "# but this doesn't really make sense\n",
        "# when we write or speak\n",
        "# we use previous values in the sentence to figure out what to say next\n",
        "# as an example if we were continuing: 'the boy is eating a blue'\n",
        "# we would probably pay attention to the word 'boy' and 'eating' and 'blue' to figure out the next token\n",
        "# and we might choose a word that related to something that a boy would eat that's blue\n",
        "# like candy"
      ],
      "metadata": {
        "id": "asdcStYXQaJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so in this sequence\n",
        "# the boy is eating a blue\n",
        "# our bigram model would perform a lookup on 'blue'\n",
        "# and output the best predicted word vector to follow it, maybe 'car'\n",
        "\n",
        "# what we want to do is a similar lookup, but across all of the vectors\n",
        "# we want to submit our query of 'blue' to each of ['the', 'boy', 'is' , ...]\n",
        "# each of these keys should then return some value its associated with, like in a lookup table\n",
        "# and we'll use that output score to determine what to pay attention to\n",
        "\n",
        "# for example\n",
        "# lookup[boy] = boy_value\n",
        "# blue * boy_value -> attn_score 0.2\n",
        "# blue * eating_value -> attn_score 0.75\n",
        "# blue * the_value -> attn_score 0.05\n",
        "\n",
        "# this might lead the model to instead try to choose a vector like 'lollipop'\n",
        "\n",
        "# by letting the model do these lookups, we give it the ability to learn how words relate to each other\n",
        "# so for each value in our sequence\n",
        "# we'll want to create a query vector, a key vector, and a value vector\n",
        "# the query vector is what each token is looking for in other tokens\n",
        "# the key vector is something for other tokens to match into\n",
        "# and the value vector is what the token represents, and is used when matched into"
      ],
      "metadata": {
        "id": "hE9jNf2BTVrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so query is of size (embed_dim)\n",
        "# and we want to compute the similarity of that query and the ith key\n",
        "# we'll use a dot product for this # TODO: EXPLAIN WHY\n",
        "# q @ k_i\n",
        "\n",
        "# we know that we'll have a maximum amount of keys, (context_len)\n",
        "# so we can actually make a matrix of keys, where the ith row is k_i\n",
        "# with (context_len) rows\n",
        "# this means instead of q @ k_i, we can just do q @ K.T (K being the matrix of k_i rows)\n",
        "\n",
        "# these dot products can get pretty fucking big\n",
        "# so we scale them down by multiplying 1/((embed_dim)**0.5)\n",
        "\n",
        "# we want these attention weights to add to 1, so we do a softmax transform on it\n",
        "# so now this final weighting allows us to extract the relevant amount of information from each point in the sequence\n",
        "# so V is of size (context_len, embed_dim)\n",
        "# and we take the dot product of softmaxed_scores @ v\n",
        "\n",
        "# in order to do all of these lookups of qs in parallel\n",
        "# we actually want Q to also be a matrix, of size (context_len, embed_dim) (technically batch_size, context_len, embed_dim)\n",
        "\n",
        "# putting it all together:\n",
        "\n",
        "\"\"\"\n",
        "def attention(q, k, v):\n",
        "\n",
        "    # q is of shape (context_len, embed_dim)\n",
        "\n",
        "    embed_dim = ... # should come from q.shape ...\n",
        "\n",
        "    attn_scores = ...\n",
        "    scaled_scores = ...\n",
        "    softmaxed_scores = ...\n",
        "\n",
        "    output = ...\n",
        "\n",
        "    return output\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "embed_dim = 8\n",
        "\n",
        "q = jax.random.normal(key, (batch_size, seq_len, embed_dim))\n",
        "k = jax.random.normal(key, (batch_size, seq_len, embed_dim))\n",
        "v = jax.random.normal(key, (batch_size, seq_len, embed_dim))\n",
        "\n",
        "batched_attention = jax.vmap(attention)\n",
        "batched_attention(q, k, v)\n",
        "\"\"\"\n",
        "\n",
        "def attention(q, k, v):\n",
        "    embed_dim = q.shape[-1] # (batch_size, context_len, embed_dim)\n",
        "\n",
        "    attn_scores = q @ k.T # (context_len, embed_dim) @ (context_len, embed_dim).T -> (context_len, context_len)\n",
        "    scaled_scores = attn_scores * 1/jnp.sqrt(embed_dim)\n",
        "    softmaxed_scores = jax.nn.softmax(scaled_scores)\n",
        "\n",
        "    output = softmaxed_scores @ v # (context_len, context_len) @ (context_len, embed_dim).T -> (context_len, embed_dim)\n",
        "\n",
        "    return output\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 4\n",
        "embed_dim = 8\n",
        "\n",
        "q = jax.random.normal(key, (batch_size, seq_len, embed_dim))\n",
        "k = jax.random.normal(key, (batch_size, seq_len, embed_dim))\n",
        "v = jax.random.normal(key, (batch_size, seq_len, embed_dim))\n",
        "\n",
        "batched_attention = jax.vmap(attention)\n",
        "batched_attention(q, k, v)"
      ],
      "metadata": {
        "id": "CoqWaQETTWbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### causal attention"
      ],
      "metadata": {
        "id": "Q9DFMX8gTczU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remember that we need to make sure we don't communicate with the future\n",
        "# so we can modify the attention matrix by adding our mask from before\n",
        "\n",
        "def create_causal_mask(seq_len: int):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "# since the scores are raw logits, we typically mask with a really large negative value, instead of using 0\n",
        "# so our mask is a lower triangular of 0s, and the rest of it is -inf\n",
        "\n",
        "# float('-inf')\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def causal_attention(q, k, v, mask):\n",
        "    embed_dim = q.shape[-1]\n",
        "\n",
        "    attn_scores = q @ k.T\n",
        "    scaled_scores = attn_scores * 1/jnp.sqrt(embed_dim)\n",
        "    masked_scores = ... # add mask, use jnp.where to check for 0s and replace with float ('-inf')\n",
        "    softmaxed_scores = jax.nn.softmax(masked_scores)\n",
        "\n",
        "    output = softmaxed_scores @ v\n",
        "\n",
        "    return output\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def causal_attention(q, k, v, mask):\n",
        "    embed_dim = q.shape[-1]\n",
        "\n",
        "    attn_scores = q @ k.T\n",
        "    scaled_scores = attn_scores / jnp.sqrt(embed_dim)\n",
        "    masked_scores = jnp.where(mask == 0, float('-inf'), scaled_scores)  # Corrected this line\n",
        "    softmaxed_scores = jax.nn.softmax(masked_scores, axis=-1)\n",
        "\n",
        "    output = softmaxed_scores @ v\n",
        "\n",
        "    return output\n",
        "\n",
        "mask = create_causal_mask(seq_len)\n",
        "batched_causal_attention = jax.vmap(causal_attention, in_axes=(0, 0, 0, None))\n",
        "batched_causal_attention(q, k, v, mask)"
      ],
      "metadata": {
        "id": "ppTBun8WTcbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### self attention"
      ],
      "metadata": {
        "id": "20upmi-bTgFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# self attention is named as such because it attends to itself\n",
        "# so each of our q, k, v values are just some input x value of shape (seq_len, embed_dim)\n",
        "# we apply weight matrices of size (embed_dim, embed_dim)\n",
        "# as well as a bias of size (embed_dim)\n",
        "# so that the model can learn how to assign q, k, v values to some input x\n",
        "# we'll call these w_q, b_q, w_k, ...\n",
        "\n",
        "# we also do a final projection at the end to add some more space to learn about the attended vectors\n",
        "\n",
        "\"\"\"\n",
        "def self_attention(x, w_q, b_q, w_k, b_k, w_v, b_v, w_proj, b_proj, causal_mask): # (seq_len, embed_dim) -> (seq_len, embed_dim)\n",
        "    seq_len = q.shape[-1]\n",
        "\n",
        "    # qkv projections\n",
        "    q = ...\n",
        "    k = ...\n",
        "    v = ...\n",
        "\n",
        "    # create mask\n",
        "    causal_mask = ...\n",
        "\n",
        "    # perform self attention\n",
        "    x = ...\n",
        "\n",
        "    # out projection\n",
        "    x = ...\n",
        "\n",
        "    return x\n",
        "\"\"\"\n",
        "\n",
        "def self_attention(x, w_q, b_q, w_k, b_k, w_v, b_v, w_proj, b_proj, causal_mask): # [seq_len, embed_dim] -> [seq_len, embed_dim]\n",
        "    seq_len = x.shape[-1]\n",
        "    # qkv projections\n",
        "    q = x @ w_q + b_q # [seq_len, embed_dim] @ [embed_dim, embed_dim] -> (seq_len, embed_dim)\n",
        "    k = x @ w_k + b_k # [seq_len, embed_dim] @ [embed_dim, embed_dim] -> (seq_len, embed_dim)\n",
        "    v = x @ w_v + b_v # [seq_len, embed_dim] @ [embed_dim, embed_dim] -> (seq_len, embed_dim)\n",
        "\n",
        "\n",
        "    # perform self attention\n",
        "    attn_scores = (q @ k.T) / jnp.sqrt(embed_dim)\n",
        "    masked_scores = jnp.where(causal_mask == 0, float('-inf'), attn_scores)\n",
        "    attn_weights = jax.nn.softmax(masked_scores, axis=-1)\n",
        "    x = attn_weights @ v\n",
        "\n",
        "    # out projection\n",
        "    x = (x @ w_proj) + b_proj  # [seq_len, embed_dim] @ [embed_dim, embed_dim] -> [seq_len, embed_dim]\n",
        "\n",
        "    return x\n",
        "\n",
        "x = jax.random.normal(key, (batch_size, seq_len, embed_dim))\n",
        "w_q = jax.random.normal(key, (embed_dim, embed_dim))\n",
        "b_q = jax.random.normal(key, (embed_dim,))\n",
        "w_k = jax.random.normal(key, (embed_dim, embed_dim))\n",
        "b_k = jax.random.normal(key, (embed_dim,))\n",
        "w_v = jax.random.normal(key, (embed_dim, embed_dim))\n",
        "b_v = jax.random.normal(key, (embed_dim,))\n",
        "w_proj = jax.random.normal(key, (embed_dim, embed_dim))\n",
        "b_proj = jax.random.normal(key, (embed_dim,))\n",
        "\n",
        "batched_self_attention1 = jax.vmap(self_attention, in_axes=(0, *([None] * 9)))\n",
        "batched_self_attention1(x, w_q, b_q, w_k, b_k, w_v, b_v, w_proj, b_proj, mask)"
      ],
      "metadata": {
        "id": "99h90oiuj6cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# typically, we store the learned weights for qkv all in the same matrix\n",
        "# and then split them up after multiplying\n",
        "# this is more efficient because multiplying one large matrix is better than 3 smaller multiplications\n",
        "# especially if on an accelerator like a GPU/TPU\n",
        "# so the shape of that attention weight matrix is (seq_len, 3*embed_dim)\n",
        "# and we split the output of that linear layer along the last axis, the embed_dim\n",
        "# to become the q/k/v for the self attention\n",
        "\n",
        "\"\"\"\n",
        "def self_attention(x, w_qkv, b_qkv, w_proj, b_proj, causal_mask): # [seq_len, embed_dim] -> [seq_len, embed_dim]\n",
        "\n",
        "    # qkv weights in one matrix multiply, add bias\n",
        "    x = ...\n",
        "\n",
        "    # split weights into qkv\n",
        "    q, k, v = ...\n",
        "\n",
        "    # perform self attention\n",
        "    x = ...\n",
        "\n",
        "    # out projection\n",
        "    x = ...\n",
        "\n",
        "    return x\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def self_attention(x, w_qkv, b_qkv, w_proj, b_proj, causal_mask):\n",
        "    seq_len, embed_dim = x.shape\n",
        "\n",
        "    # qkv projections\n",
        "    x = (x @ w_qkv) + b_qkv  # [seq_len, embed_dim] @ [embed_dim, 3*embed_dim] -> [seq_len, 3*embed_dim]\n",
        "\n",
        "    # split into qkv\n",
        "    q, k, v = jnp.split(x, 3, axis=-1)  # [seq_len, 3*embed_dim] -> 3 of [seq_len, embed_dim]\n",
        "\n",
        "    # perform self attention\n",
        "    attn_scores = (q @ k.T) / jnp.sqrt(embed_dim)\n",
        "    masked_scores = jnp.where(causal_mask == 0, float('-inf'), attn_scores)\n",
        "    attn_weights = jax.nn.softmax(masked_scores, axis=-1)\n",
        "    x = attn_weights @ v\n",
        "\n",
        "    # out projection\n",
        "    x = (x @ w_proj) + b_proj  # [seq_len, embed_dim] @ [embed_dim, embed_dim] -> [seq_len, embed_dim]\n",
        "\n",
        "    return x\n",
        "\n",
        "w_qkv = jax.random.normal(key, (embed_dim, 3 * embed_dim))\n",
        "b_qkv = jax.random.normal(key, (3 * embed_dim,))\n",
        "\n",
        "batched_self_attention = jax.vmap(self_attention, in_axes=(0, *([None] * 5)))\n",
        "batched_self_attention(x, w_qkv, b_qkv, w_proj, b_proj, mask)"
      ],
      "metadata": {
        "id": "etr2Cz3dTfp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### multi head attention"
      ],
      "metadata": {
        "id": "XSpKzkS0YDg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# as one last final complication, remember that each part of a sentence carries high dimensional information\n",
        "# 'the boy is eating a'\n",
        "# we want to generate the next token based on the semantics of 'eating' and it being a food\n",
        "# we want to have it be a singular item, because of the 'a'\n",
        "# so we may want to pay different kinds of attention to different tokens, depending on syntax/grammar/semantics\n",
        "\n",
        "# this leads us to multi head attention\n",
        "# to maintain the computational complexity\n",
        "# we downscale the Q/K/V matrices from embed_dim down to embed_dim//n_heads (head_dim)\n",
        "#\n",
        "# and we end up with multiple matrices Q_i, K_i, V_i for each ith head\n",
        "# note that this is a tradeoff! we sacrifice some dimensionality for each token but gain ensembling power"
      ],
      "metadata": {
        "id": "bvPnaut_VY_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll start by doing the linear layer as before\n",
        "# which will give us a shape of (seq_len, 3*embed_dim)\n",
        "# then, we need to split into q/k/v, as well as split along each head\n",
        "# we want to end up with shape (seq_len, 3, n_head, head_dim)\n",
        "# so we can just use a .reshape to get there\n",
        "# then, we can split this tensor along that second dimension we specified earlier\n",
        "# now x_q, x_k, x_v are tensors of shape (seq_len, n_head, head_dim)\n",
        "# we need to perform q @ k.T\n",
        "# and then @ v\n",
        "# and we want to do it along the n heads\n",
        "# so we need to transpose our tensors into (n_head, seq_len, head_dim)\n",
        "# note that while doing this reshaping, we can actually do the transpose on k\n",
        "# and transpose k to (n_head, head_dim, seq_len) instead\n",
        "# then, we do the matrix multiplication, scale by head_dim, mask and softmax\n",
        "# to get our context weightings\n",
        "# remember that our context weightings are of shape (n_head, seq_len, head_dim)\n",
        "# and we want to go back to (seq_len, n_head, head_dim)\n",
        "# so we do the .transpose again\n",
        "# finally, we need to put the n_head/head_dim back together before our final linear layer\n",
        "# so we .reshape into (seq_len, embed_dim)\n",
        "# which concatenates the heads back together\n",
        "# and do the final output projection\n",
        "# [seq_len, n_head, head_dim]\n",
        "\n",
        "\"\"\"\n",
        "def multi_head_attn(x: jax.Array, w_qkv: jax.Array, b_qkv: jax.Array, w_proj: jax.Array, b_proj: jax.Array, n_head: int, causal_mask: jax.Array):\n",
        "    seq_len, embed_dim = ...\n",
        "    head_dim = ...\n",
        "\n",
        "    # linear projections\n",
        "    x_qkv = ...\n",
        "    x_qkv_heads = ... # split into heads\n",
        "    xq, xk, xv = ... # indices [0, 1, 2] respectively on the dim we just created\n",
        "\n",
        "    # reshape for attention\n",
        "    xq = ...\n",
        "    xkt = ...\n",
        "    xv = ...\n",
        "\n",
        "    # create search with q/k\n",
        "    raw_scores = ...\n",
        "    scaled_scores = ...\n",
        "    masked_scores = ...\n",
        "    attn_weights = ...\n",
        "\n",
        "    # apply search with v to get final contextual weights\n",
        "    context_weights = ...\n",
        "\n",
        "    # transpose and reshape\n",
        "    transposed_context_weights = ...\n",
        "    reshaped_context_weights = ...\n",
        "\n",
        "    # project vectors back to tokens\n",
        "    token_logits = ...\n",
        "\n",
        "    return token_logits\n",
        "\"\"\"\n",
        "\n",
        "def multi_head_attn(x: jax.Array, w_qkv: jax.Array, b_qkv: jax.Array, w_proj: jax.Array, b_proj: jax.Array, n_head: int, causal_mask: jax.Array):\n",
        "    seq_len, embed_dim = x.shape\n",
        "    head_dim = embed_dim // n_head\n",
        "\n",
        "    x_qkv = jnp.dot(x, w_qkv) + b_qkv\n",
        "    x_qkv_heads = x_qkv.reshape(seq_len, 3, n_head, head_dim)\n",
        "    xq, xk, xv = x_qkv_heads[:, 0], x_qkv_heads[:, 1], x_qkv_heads[:, 2]\n",
        "\n",
        "    xq = xq.transpose(1, 0, 2)  # (n_head, seq_len, head_dim)\n",
        "    xkt = xk.transpose(1, 2, 0)  # (n_head, head_dim, seq_len)\n",
        "    xv = xv.transpose(1, 0, 2)  # (n_head, seq_len, head_dim)\n",
        "\n",
        "    raw_scores = jnp.matmul(xq, xkt)/ jnp.sqrt(head_dim)\n",
        "    scaled_scores = raw_scores\n",
        "    masked_scores = jnp.where(causal_mask == 0, float('-inf'), scaled_scores)\n",
        "    attn_weights = jax.nn.softmax(masked_scores, axis=-1)\n",
        "\n",
        "    context_weights = jnp.matmul(attn_weights, xv)\n",
        "\n",
        "    transposed_context_weights = context_weights.transpose(1, 0, 2)\n",
        "    reshaped_context_weights = transposed_context_weights.reshape(seq_len, embed_dim)\n",
        "\n",
        "    token_logits = jnp.dot(reshaped_context_weights, w_proj) + b_proj\n",
        "\n",
        "    return token_logits\n",
        "\n",
        "n_head = 2\n",
        "batched_multi_head_attn = jax.vmap(multi_head_attn, in_axes=(0, *([None] * 6)))\n",
        "batched_multi_head_attn(x, w_qkv, b_qkv, w_proj, b_proj, n_head, mask)"
      ],
      "metadata": {
        "id": "AG7ewrbspYVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make our first model that actually puts this into practice!\n",
        "# make sure to initialize the parameters with the correct shapes\n",
        "\n",
        "class ModelConfig(NamedTuple):\n",
        "    vocab_size: int\n",
        "    embedding_dim: int\n",
        "    context_len: int\n",
        "    n_head: int\n",
        "\n",
        "\"\"\"\n",
        "def init_attentive_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    k1, k2, k3, k4, k5, k6, k7 = jax.random.split(key, 7)\n",
        "    return {\n",
        "        'token_embedding': jax.random.normal(k1, (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': jax.random.normal(k2, (model_config.context_len, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(k3, (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor,\n",
        "        'attn_in_weights': ...,\n",
        "        'attn_in_bias': ...,\n",
        "        'attn_out_weights': ...,\n",
        "        'attn_out_bias': ...,\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def init_attentive_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    k1, k2, k3, k4, k5, k6, k7 = jax.random.split(key, 7)\n",
        "    return {\n",
        "        'token_embedding': jax.random.normal(k1, (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': jax.random.normal(k2, (model_config.context_len, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(k3, (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor,\n",
        "        'attn_in_weights': jax.random.normal(k4, (model_config.embedding_dim, 3*model_config.embedding_dim)) * scaling_factor,\n",
        "        'attn_in_bias': jax.random.normal(k5, (3*model_config.embedding_dim,)) * scaling_factor,\n",
        "        'attn_out_weights': jax.random.normal(k6, (model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "        'attn_out_bias': jax.random.normal(k7, (model_config.embedding_dim,)) * scaling_factor\n",
        "    }"
      ],
      "metadata": {
        "id": "igZwam9iuobs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the forward pass\n",
        "# after embedding the tokens\n",
        "# create the causal mask\n",
        "# apply the multi head attention\n",
        "# then project the vectors back into tokens\n",
        "\n",
        "\"\"\"\n",
        "def attentive_model(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = tokens.shape[0]\n",
        "\n",
        "    # embed tokens\n",
        "    token_embeds = ...\n",
        "    pos_embeds = ...\n",
        "    embedded = ...\n",
        "\n",
        "\n",
        "    # create causal mask\n",
        "    causal_mask = ...\n",
        "\n",
        "    # apply multi-head attention\n",
        "    context = ...\n",
        "\n",
        "    # project vectors back to tokens\n",
        "    token_logits = ...\n",
        "\n",
        "    return token_logits\n",
        "\"\"\"\n",
        "\n",
        "def attentive_model(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = tokens.shape[0]\n",
        "\n",
        "\n",
        "    token_embeds = model_params['token_embedding'][tokens]\n",
        "    pos_embeds = model_params['positional_embedding'][:seq_len]\n",
        "    embeds = token_embeds + pos_embeds\n",
        "\n",
        "    causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "\n",
        "    context = multi_head_attn(embeds, model_params['attn_in_weights'], model_params['attn_in_bias'], model_params['attn_out_weights'],\n",
        "                        model_params['attn_out_bias'], model_config.n_head, causal_mask)\n",
        "\n",
        "    token_logits = jnp.dot(context, model_params['output_projection'])\n",
        "\n",
        "    return token_logits"
      ],
      "metadata": {
        "id": "ND0qFzboTjdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once again, we train this model\n",
        "\n",
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed,\n",
        "    'causal_embed': causal_embed,\n",
        "    'causal_pos': causal_pos,\n",
        "    'attentive_model': attentive_model,\n",
        "}\n",
        "\n",
        "train_config = TrainConfig(num_epochs=50, batches_per_epoch=32, batch_size=32, batch_seq_len=64, seed=0)\n",
        "adamw_config = OptConfig(lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01, opt_init=init_adam_state, opt_update=adamw_update)\n",
        "attn_config = ModelConfig(vocab_size=95, embedding_dim=32, context_len=128, n_head=4)\n",
        "attn_params = init_attentive_params(key, attn_config)\n",
        "trained_attn_params = train(attn_params, attn_config, 'attentive_model', train_tokens, adamw_config, train_config)\n",
        "compare_params(prompt, attn_config, 'attentive_model', attn_params, trained_attn_params)\n",
        "compare_params(shuffled_prompt, attn_config, 'attentive_model', attn_params, trained_attn_params)"
      ],
      "metadata": {
        "id": "jHDuHHn9U8tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### feed forward networks"
      ],
      "metadata": {
        "id": "hXgc3ynkwjVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# while we've given the model a way to understand context\n",
        "# and it successfully produces different output for a shuffled prompt\n",
        "# its learning context of what tokens to pay attention to\n",
        "# but it doesnt really process that information in any way\n",
        "# we can take the outputs and process them by using a basic feedforward network\n",
        "# when a feedforward network processes mnist handwritten data\n",
        "# it goes from understanding the vector representation of the grayscale pixels\n",
        "# to enough information to classify them into digits\n",
        "\n",
        "# similarly, taking the output of the attention's contextualized representation of the positionally encoded token vectors\n",
        "# the feedforward network 'thinks about it' and produces a more useful output\n",
        "\n",
        "# our network will expand into a 4x dimensional space from its input\n",
        "# to have more 'space' to work with\n",
        "# go through an activation function\n",
        "# to understand nonlinear relationships\n",
        "# and shrink back down into the original dimension of the token embeddings\n",
        "# forcing it to compress what its learned into the most useful stuff\n"
      ],
      "metadata": {
        "id": "QalWQ56Gcb4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# each layer gets a weight matrix and a bias matrix\n",
        "# the input gets dotted with the weight, then you add the bias\n",
        "# just like we've seen many times above\n",
        "# the weight/bias are kind of like the slope/intercept in a regression\n",
        "# the slope does a lot of heavy lifting in the understanding of relationships\n",
        "# and the weight means it doesn't have to start from 0, and helps get the vectors into the right place\n",
        "\n",
        "\"\"\"\n",
        "def linear(x, weight, bias):  # [m, in], [in, out], [out] -> [m, out]\n",
        "    return\n",
        "\"\"\"\n",
        "\n",
        "def linear(x, weight, bias):  # [m, in], [in, out], [out] -> [m, out]\n",
        "    return x @ weight + bias\n",
        "\n",
        "in_dim = embed_dim\n",
        "out_dim = 2 * embed_dim\n",
        "weight = jax.random.normal(key, (in_dim, out_dim))\n",
        "bias = jax.random.normal(key, (out_dim,))\n",
        "linear(x[0], weight, bias)"
      ],
      "metadata": {
        "id": "AsCa4L9iv4D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to add nonlinearity to this network\n",
        "# we use an activation function that is not linear\n",
        "# typically something like relu (f(x) = max(0,x)) is used for this\n",
        "# id love to write an intuitive reason for the choice of activation function for gpt2\n",
        "# but it was actually just chosen bc it works empirically\n",
        "# the activation function is applied after each linear layer, except the last one\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 +jnp.tanh(jnp.sqrt(2 /jnp.pi) * (x + 0.044715 * x**3)))"
      ],
      "metadata": {
        "id": "F6qWYjTQwVP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so now our network should take in two sets of linear layer parameters\n",
        "# one for projecting up in to the higher dimensional space\n",
        "# and one for projecting back down\n",
        "\n",
        "\"\"\"\n",
        "def ffn(x, c_in_params, c_out_params):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # project up\n",
        "    projected_up = ...\n",
        "\n",
        "    # project back down\n",
        "    output = ...\n",
        "\n",
        "    return output\n",
        "\"\"\"\n",
        "\n",
        "def ffn(x, c_in_weight, c_in_bias, c_out_weight, c_out_bias):  # [n_seq, n_embd] -> [n_seq, n_embd]\n",
        "    # project up\n",
        "    projected_up = gelu(linear(x, c_in_weight, c_in_bias))  # [n_seq, n_embd] -> [n_seq, 4*n_embd]\n",
        "\n",
        "    # project back down\n",
        "    output = linear(projected_up, c_out_weight, c_out_bias)  # [n_seq, 4*n_embd] -> [n_seq, n_embd]\n",
        "\n",
        "    return output\n",
        "\n",
        "c_in_weight = jax.random.normal(key, (embed_dim, 4 * embed_dim))\n",
        "c_in_bias = jax.random.normal(key, (4 * embed_dim,))\n",
        "c_out_weight = jax.random.normal(key, (4 * embed_dim, embed_dim))\n",
        "c_out_bias = jax.random.normal(key, (embed_dim,))\n",
        "ffn(x[0], c_in_weight, c_in_bias, c_out_weight, c_out_bias)"
      ],
      "metadata": {
        "id": "fsjlmbAxyvm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll clean up our parameters a little bit while we're here\n",
        "# we can use nested dictionaries for each of our linear layers\n",
        "\n",
        "class ModelConfig(NamedTuple):\n",
        "    vocab_size: int\n",
        "    embedding_dim: int\n",
        "    context_len: int\n",
        "    n_head: int\n",
        "\n",
        "\"\"\"\n",
        "def init_attentive_ffn_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    key, *subkeys  = jax.random.split(key, 12)\n",
        "    return {\n",
        "        'token_embedding': jax.random.normal(subkeys[0], (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': jax.random.normal(subkeys[1], (model_config.context_len, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(subkeys[2], (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor,\n",
        "        'attn_in': {\n",
        "            'weight': jax.random.normal(subkeys[3], (model_config.embedding_dim, 3*model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[4], (3*model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'attn_out': {\n",
        "            'weight': jax.random.normal(subkeys[5], (model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[6], (model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'ffn_in': {},\n",
        "        'ffn_out': {},\n",
        "\n",
        "    }\n",
        "\"\"\"\n",
        "\n",
        "def init_attentive_ffn_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    key, *subkeys  = jax.random.split(key, 12)\n",
        "    return {\n",
        "        'token_embedding': jax.random.normal(subkeys[0], (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': jax.random.normal(subkeys[1], (model_config.context_len, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(subkeys[2], (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor,\n",
        "        'attn_in': {\n",
        "            'weight': jax.random.normal(subkeys[3], (model_config.embedding_dim, 3*model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[4], (3*model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'attn_out': {\n",
        "            'weight': jax.random.normal(subkeys[5], (model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[6], (model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'ffn_in': {\n",
        "            'weight': jax.random.normal(subkeys[7], (model_config.embedding_dim, 4*model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[8], (4*model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'ffn_out': {\n",
        "            'weight': jax.random.normal(subkeys[9], (4*model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[10], (model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "\n",
        "    }\n"
      ],
      "metadata": {
        "id": "WvN-NwwJxVBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the forward pass\n",
        "# just add in the ffn function we defined earlier\n",
        "# right after the multi head attention\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def attentive_ffn(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = ...\n",
        "    causal_mask = ...\n",
        "\n",
        "    token_embeds = ...\n",
        "    pos_embeds = ...\n",
        "    embedded = ...\n",
        "\n",
        "\n",
        "    context = ...\n",
        "\n",
        "    enhanced_context = ...\n",
        "\n",
        "    token_logits = ...\n",
        "\n",
        "    return token_logits\n",
        "\"\"\"\n",
        "\n",
        "def attentive_ffn(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = tokens.shape[0]\n",
        "    causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "    token_embeds = model_params['token_embedding'][tokens]\n",
        "    pos_embeds = model_params['positional_embedding'][:seq_len]\n",
        "    embeds = token_embeds + pos_embeds\n",
        "\n",
        "\n",
        "    context = multi_head_attn(embeds, model_params['attn_in']['weight'], model_params['attn_in']['bias'], model_params['attn_out']['weight'],\n",
        "                        model_params['attn_out']['bias'], model_config.n_head, causal_mask)\n",
        "\n",
        "    enhanced_context = ffn(context, model_params['ffn_in']['weight'], model_params['ffn_in']['bias'], model_params['ffn_out']['weight'], model_params['ffn_out']['bias'])\n",
        "\n",
        "    token_logits = jnp.dot(enhanced_context, model_params['output_projection'])\n",
        "\n",
        "    return token_logits"
      ],
      "metadata": {
        "id": "GqEMR8Q1DdX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed,\n",
        "    'causal_embed': causal_embed,\n",
        "    'causal_pos': causal_pos,\n",
        "    'attentive_model': attentive_model,\n",
        "    'attentive_ffn': attentive_ffn\n",
        "}\n",
        "\n",
        "train_config = TrainConfig(num_epochs=50, batches_per_epoch=32, batch_size=32, batch_seq_len=64, seed=0)\n",
        "adamw_config = OptConfig(lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.01, opt_init=init_adam_state, opt_update=adamw_update)\n",
        "attn_config = ModelConfig(vocab_size=95, embedding_dim=32, context_len=128, n_head=4)\n",
        "attn_ffn = init_attentive_params(key, attn_config)\n",
        "trained_attn_ffn_params = train(attn_ffn, attn_config, 'attentive_model', train_tokens, adamw_config, train_config)\n",
        "compare_params(prompt, attn_config, 'attentive_model', attn_ffn, trained_attn_ffn_params)\n"
      ],
      "metadata": {
        "id": "jqZ7j7hu0CCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### layer normalization"
      ],
      "metadata": {
        "id": "EmbQPnD4wiEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# while we gave the model room to think more\n",
        "# we're starting to get a pretty deep network\n",
        "# as evidenced by our higher ending loss\n",
        "# the model immediately has to start processing the attention output in the feedforward network\n",
        "# we can think of it as the model needing to recalibrate before moving onto the next task\n",
        "# a little more mathematically, we might be getting very skewed or weirdly distributed values from the attention output\n",
        "# so we want a way to stabilize the input that each of our functions gets\n",
        "# the most obvious way to do this is to normalize the distribution, to have mean 0 and variance 1\n",
        "# this may not be the best way to represent the data to the next function\n",
        "# so we also give the model a learnable scale/shift parameter\n",
        "# so it can affine transform the distribution as needed\n",
        "\n",
        "# when writing the layer normalization\n",
        "# we can use jnp mean, jnp var, etc\n",
        "# be sure to use the axis and keepdims parameters\n",
        "# to make sure we take the mean across the correct axis\n",
        "# we want to take the mean/variance over the last axis (the one where the features are)\n",
        "# and we want to keep the dimensions after the processing\n",
        "\n",
        "\"\"\"\n",
        "def layer_norm(x, gamma, beta, eps: float = 1e-5):\n",
        "    mean = ...\n",
        "    variance = ...\n",
        "    normalized_x = ...  # normalize x to have mean=0 and var=1 over last axis\n",
        "    affine_x = ...\n",
        "    return affine_x\n",
        "\"\"\"\n",
        "\n",
        "def layer_norm(x, gamma, beta, eps: float = 1e-5):\n",
        "    mean = jnp.mean(x, axis=-1, keepdims=True)\n",
        "    variance = jnp.var(x, axis=-1, keepdims=True)\n",
        "    normalized_x = (x - mean) / jnp.sqrt(variance + eps)  # normalize x to have mean=0 and var=1 over last axis\n",
        "    affine_x = gamma * normalized_x + beta\n",
        "    return affine_x\n",
        "\n",
        "gamma = jnp.ones((embed_dim,))\n",
        "beta = jnp.zeros((embed_dim,))\n",
        "layer_norm_output = layer_norm(x[0], gamma, beta)\n",
        "jnp.mean(layer_norm_output, axis=-1, keepdims=True), jnp.var(layer_norm_output, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "vXTlD2m2lZFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in our model\n",
        "# we'll want to have a layer norm before the attention output\n",
        "# a layer norm before the feedforward\n",
        "# and a final layer norm before we project back into token space\n",
        "# you might not want to initialize the gamma/beta from the normal distribution ... whats a sensible default (jnp.zeros/jnp.ones?)\n",
        "\n",
        "\"\"\"\n",
        "def init_attentive_ffn_ln_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    key, *subkeys = jax.random.split(key, 16)\n",
        "    return {\n",
        "        'token_embedding': jax.random.normal(subkeys[0], (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': jax.random.normal(subkeys[1], (model_config.context_len, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(subkeys[2], (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor,\n",
        "        'attn_in': {\n",
        "            'weight': jax.random.normal(subkeys[3], (model_config.embedding_dim, 3*model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[4], (3*model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'attn_out': {\n",
        "            'weight': jax.random.normal(subkeys[5], (model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[6], (model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'ffn_in': {\n",
        "            'weight': jax.random.normal(subkeys[7], (model_config.embedding_dim, 4*model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[8], (4*model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'ffn_out': {\n",
        "            'weight': jax.random.normal(subkeys[9], (4*model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[10], (model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'ln1': {},\n",
        "        'ln2': {},\n",
        "        'lnf': {},\n",
        "    }\n",
        "\"\"\"\n",
        "def init_attentive_ffn_ln_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    key, *subkeys = jax.random.split(key, 16)\n",
        "    return {\n",
        "        'token_embedding': jax.random.normal(subkeys[0], (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': jax.random.normal(subkeys[1], (model_config.context_len, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(subkeys[2], (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor,\n",
        "        'attn_in': {\n",
        "            'weight': jax.random.normal(subkeys[3], (model_config.embedding_dim, 3*model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[4], (3*model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'attn_out': {\n",
        "            'weight': jax.random.normal(subkeys[5], (model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[6], (model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'ffn_in': {\n",
        "            'weight': jax.random.normal(subkeys[7], (model_config.embedding_dim, 4*model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[8], (4*model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'ffn_out': {\n",
        "            'weight': jax.random.normal(subkeys[9], (4*model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "            'bias': jax.random.normal(subkeys[10], (model_config.embedding_dim,)) * scaling_factor,\n",
        "        },\n",
        "        'ln1': {\n",
        "            'gamma': jnp.ones((model_config.embedding_dim,)),\n",
        "            'beta': jnp.zeros((model_config.embedding_dim,)),\n",
        "        },\n",
        "        'ln2': {\n",
        "            'gamma': jnp.ones((model_config.embedding_dim,)),\n",
        "            'beta': jnp.zeros((model_config.embedding_dim,)),\n",
        "        },\n",
        "        'lnf': {\n",
        "            'gamma': jnp.ones((model_config.embedding_dim,)),\n",
        "            'beta': jnp.zeros((model_config.embedding_dim,)),\n",
        "        },\n",
        "    }"
      ],
      "metadata": {
        "id": "wmgyyOt4GGi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for the forward pass\n",
        "# add in the layer norms as we discussed above\n",
        "\n",
        "\"\"\"\n",
        "def attentive_ffn_ln(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = ...\n",
        "    causal_mask = ...\n",
        "    token_embeds = ...\n",
        "    pos_embeds = ...\n",
        "    embedded = ...\n",
        "    normed_embedded = ...\n",
        "\n",
        "\n",
        "\n",
        "    context = ...\n",
        "    normed_context = ...\n",
        "\n",
        "    enhanced_context = ...\n",
        "    normed_enhanced_context = ...\n",
        "\n",
        "    token_logits = ...\n",
        "\n",
        "    return token_logits\n",
        "\"\"\"\n",
        "\n",
        "def attentive_ffn_ln(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = tokens.shape[0]\n",
        "    causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "    token_embeds = model_params['token_embedding'][tokens]\n",
        "    pos_embeds = model_params['positional_embedding'][:seq_len]\n",
        "    embedded = token_embeds + pos_embeds\n",
        "\n",
        "\n",
        "\n",
        "    normed_embedded = layer_norm(embedded, model_params['ln1']['gamma'], model_params['ln1']['beta'])\n",
        "\n",
        "    context = multi_head_attn(normed_embedded, model_params['attn_in']['weight'], model_params['attn_in']['bias'], model_params['attn_out']['weight'],\n",
        "                        model_params['attn_out']['bias'], model_config.n_head, causal_mask)\n",
        "\n",
        "    normed_context = layer_norm(context, model_params['ln2']['gamma'], model_params['ln2']['beta'])\n",
        "\n",
        "    enhanced_context = ffn(normed_context, model_params['ffn_in']['weight'], model_params['ffn_in']['bias'], model_params['ffn_out']['weight'], model_params['ffn_out']['bias'])\n",
        "\n",
        "    normed_enhanced_context = layer_norm(enhanced_context, model_params['lnf']['gamma'], model_params['lnf']['beta'])\n",
        "\n",
        "    token_logits = jnp.dot(normed_enhanced_context, model_params['output_projection'])\n",
        "\n",
        "    return token_logits"
      ],
      "metadata": {
        "id": "U4Qo0vNiGSmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed,\n",
        "    'causal_embed': causal_embed,\n",
        "    'causal_pos': causal_pos,\n",
        "    'attentive_model': attentive_model,\n",
        "    'attentive_ffn': attentive_ffn,\n",
        "    'attentive_ffn_ln': attentive_ffn_ln,\n",
        "}\n",
        "\n",
        "attn_ffn_ln_params = init_attentive_ffn_ln_params(key, attn_config)\n",
        "trained_attn_ffn_ln_params = train(attn_ffn_ln_params, attn_config, 'attentive_ffn_ln', train_tokens, adamw_config, train_config)\n",
        "compare_params(prompt, attn_config, 'attentive_ffn_ln', attn_ffn_ln_params, trained_attn_ffn_ln_params)"
      ],
      "metadata": {
        "id": "osTUC_FXsFki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### residual connections"
      ],
      "metadata": {
        "id": "0aoLY5X3wg7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is great! our outputs are getting more and more sensible\n",
        "# but we're still not quite there yet\n",
        "# as our model gets larger and deeper\n",
        "# inputs have to travel further and further\n",
        "# we've alleviated this a bit with the layer normalizations\n",
        "# but its not enough\n",
        "# computer vision researchers found that after a certain point, adding more depth doesn't really work\n",
        "# suppose we have a model that is some n layers deep\n",
        "# we would assume that a model n+1 layers deep should always be at least as good as the n layer model\n",
        "# after all, it could just replicate the n layer deep function\n",
        "# and use the identity function in the excess layer\n",
        "# but empirically, training deeper networks actually had higher training loss\n",
        "\n",
        "# a rough intuition for this is that as we go through these complex transformations\n",
        "# our initial weights can't learn easily from the gradients we got from the input we trained on\n",
        "# its hard for the model to both preserve complex transformations, as well as the original data\n",
        "# imagine if you played a game of telephone, where the first person asks a question about the group of people\n",
        "# and the last person has to answer it, with everyone\n",
        "# not only will the initial question get lost\n",
        "# but everyone along the way has to add their information to that question, further obscuring the information\n",
        "# so we need a way to pass information through the network that isn't strictly sequential\n",
        "\n",
        "# this is because learning the identity function is actually not that easy\n",
        "# we're trying to estimate some H(x)\n",
        "# by doing H(x) = a(b(c(d(x))))\n",
        "# since each of these functions is nonlinear\n",
        "# its pretty hard to preserve x, if H(x) or one of these subfunctions needs to be f(x)=x\n",
        "# so we can reformulate the problem as trying to predict\n",
        "# H(x) = F(x) + x\n",
        "# now each step can just predict the difference that needs to be applied to x\n",
        "# and it becomes easy to just do nothing to the input\n",
        "\n",
        "# we basically just save the input before we do our layernorm/attention/ffn\n",
        "# and add it to whatever the function's output was\n",
        "# like we saw in the above example"
      ],
      "metadata": {
        "id": "MxxKQOkTvSQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "def attentive_ffn_ln_res(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = ...\n",
        "    causal_mask = ...\n",
        "\n",
        "    token_embeds = ...\n",
        "    pos_embeds = ...\n",
        "\n",
        "    embedded = ...\n",
        "    embedded_residual = ...\n",
        "    normed_embedded = ...\n",
        "\n",
        "    context = ...\n",
        "\n",
        "    context = ... # add residual\n",
        "\n",
        "    context_residual = ...\n",
        "\n",
        "    normed_context = ...\n",
        "\n",
        "    enhanced_context = ...\n",
        "\n",
        "    enhanced_context = ... # add residual\n",
        "\n",
        "    normed_enhanced_context = ...\n",
        "\n",
        "    token_logits = ...\n",
        "\n",
        "    return token_logit\n",
        "\n",
        "\"\"\"\n",
        "def attentive_ffn_ln_res(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = tokens.shape[0]\n",
        "    causal_mask = jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "    token_embeds = model_params['token_embedding'][tokens]\n",
        "    pos_embeds = model_params['positional_embedding'][:seq_len]\n",
        "\n",
        "    embedded = token_embeds + pos_embeds\n",
        "    embedded_residual = embedded\n",
        "    normed_embedded = layer_norm(embedded, model_params['ln1']['gamma'], model_params['ln1']['beta'])\n",
        "\n",
        "    context = multi_head_attn(normed_embedded, model_params['attn_in']['weight'], model_params['attn_in']['bias'], model_params['attn_out']['weight'],\n",
        "                        model_params['attn_out']['bias'], model_config.n_head, causal_mask)\n",
        "    context = context + embedded_residual\n",
        "\n",
        "    context_residual = context\n",
        "\n",
        "    normed_context = layer_norm(context, model_params['ln2']['gamma'], model_params['ln2']['beta'])\n",
        "\n",
        "    enhanced_context = ffn(normed_context, model_params['ffn_in']['weight'], model_params['ffn_in']['bias'], model_params['ffn_out']['weight'], model_params['ffn_out']['bias'])\n",
        "\n",
        "    enhanced_context = enhanced_context + context_residual\n",
        "\n",
        "    normed_enhanced_context = layer_norm(enhanced_context, model_params['lnf']['gamma'], model_params['lnf']['beta'])\n",
        "\n",
        "    token_logits = jnp.dot(normed_enhanced_context, model_params['output_projection'])\n",
        "\n",
        "    return token_logits\n",
        "\n"
      ],
      "metadata": {
        "id": "GUEabnjg0kH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed,\n",
        "    'causal_embed': causal_embed,\n",
        "    'causal_pos': causal_pos,\n",
        "    'attentive_model': attentive_model,\n",
        "    'attentive_ffn': attentive_ffn,\n",
        "    'attentive_ffn_ln': attentive_ffn_ln,\n",
        "    'attentive_ffn_ln_res': attentive_ffn_ln_res,\n",
        "\n",
        "}\n",
        "\n",
        "attn_ffn_ln_params = init_attentive_ffn_ln_params(key, attn_config)\n",
        "trained_attn_ffn_ln_params = train(attn_ffn_ln_params, attn_config, 'attentive_ffn_ln_res', train_tokens, adamw_config, train_config)\n",
        "compare_params(prompt, attn_config, 'attentive_ffn_ln_res', attn_ffn_ln_params, trained_attn_ffn_ln_params)"
      ],
      "metadata": {
        "id": "hpS1wVtf0zFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformer blocks"
      ],
      "metadata": {
        "id": "EiHapBDRwlC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the model still isn't fully there yet\n",
        "# while we now have a powerful structure\n",
        "# that can understand context and process on that context\n",
        "# we want to give it even more space to learn\n",
        "# so we can repeat the attention and the feedforward network\n",
        "# what this does is it allows the model to process the input in multiple passes\n",
        "# kind of like reading a textbook in multiple passes\n",
        "# the first pass might be like skimming over the material, looking at headings and subheadings\n",
        "# the second pass might be a closer look, reading more deeply into the important parts\n",
        "# and the final pass could be a deep reading of the book, made better by the abstract understanding from previous passes\n",
        "\n",
        "# so we'll define a block of a transformer: the residuals and layer norms around the attention/feedforward\n",
        "# and pass our input through multiple of these blocks"
      ],
      "metadata": {
        "id": "5PDOfjoXSufw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig(NamedTuple):\n",
        "    vocab_size: int\n",
        "    embedding_dim: int\n",
        "    context_len: int\n",
        "    n_head: int\n",
        "    n_blocks: int\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def init_multilayer_transformer_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    keys = jax.random.split(key, ) # use n blocks to figure out how many keys you need\n",
        "\n",
        "    params = {\n",
        "        'token_embedding': jax.random.normal(keys[0], (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': jax.random.normal(keys[1], (model_config.context_len, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(keys[2], (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor,\n",
        "        'lnf': {\n",
        "            'weight': jnp.ones((model_config.embedding_dim,)),\n",
        "            'bias': jnp.zeros((model_config.embedding_dim,)),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    for i in range(model_config.n_blocks):\n",
        "        block_key_start = ... # figure out the correct indexing strategy here\n",
        "        params[f'block_{i}'] = {\n",
        "            'attn_in': {\n",
        "                'weight': jax.random.normal(keys[block_key_start], (model_config.embedding_dim, 3*model_config.embedding_dim)) * scaling_factor,\n",
        "                'bias': jax.random.normal(keys[block_key_start+1], (3*model_config.embedding_dim,)) * scaling_factor,\n",
        "            },\n",
        "            'attn_out': {\n",
        "                'weight': jax.random.normal(keys[block_key_start+2], (model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "                'bias': jax.random.normal(keys[block_key_start+3], (model_config.embedding_dim,)) * scaling_factor,\n",
        "            },\n",
        "            'ln1': {\n",
        "                'weight': jnp.ones((model_config.embedding_dim,)),\n",
        "                'bias': jnp.zeros((model_config.embedding_dim,)),\n",
        "            },\n",
        "            'ln2': {\n",
        "                'weight': jnp.ones((model_config.embedding_dim,)),\n",
        "                'bias': jnp.zeros((model_config.embedding_dim,)),\n",
        "            },\n",
        "            'ffn_in': {\n",
        "                'weight': jax.random.normal(keys[block_key_start+4], (model_config.embedding_dim, 4*model_config.embedding_dim)) * scaling_factor,\n",
        "                'bias': jax.random.normal(keys[block_key_start+5], (4*model_config.embedding_dim,)) * scaling_factor,\n",
        "            },\n",
        "            'ffn_out': {\n",
        "                'weight': jax.random.normal(keys[block_key_start+6], (4*model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "                'bias': jax.random.normal(keys[block_key_start+7], (model_config.embedding_dim,)) * scaling_factor,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    return params\n",
        "\"\"\"\n",
        "def init_multilayer_transformer_params(key: jax.random.PRNGKey, model_config: ModelConfig, scaling_factor = 0.02) -> Dict:\n",
        "    keys = jax.random.split(key, 7 + 10 * model_config.n_blocks)\n",
        "\n",
        "    params = {\n",
        "        'token_embedding': jax.random.normal(keys[0], (model_config.vocab_size, model_config.embedding_dim)) * scaling_factor,\n",
        "        'positional_embedding': jax.random.normal(keys[1], (model_config.context_len, model_config.embedding_dim)) * scaling_factor,\n",
        "        'output_projection': jax.random.normal(keys[2], (model_config.embedding_dim, model_config.vocab_size)) * scaling_factor,\n",
        "        'lnf': {\n",
        "            'gamma': jnp.ones((model_config.embedding_dim,)),\n",
        "            'beta': jnp.zeros((model_config.embedding_dim,)),\n",
        "        },\n",
        "    }\n",
        "\n",
        "    for i in range(model_config.n_blocks):\n",
        "        block_key_start = 3 + i * 10\n",
        "        params[f'block_{i}'] = {\n",
        "            'attn_in': {\n",
        "                'weight': jax.random.normal(keys[block_key_start], (model_config.embedding_dim, 3*model_config.embedding_dim)) * scaling_factor,\n",
        "                'bias': jax.random.normal(keys[block_key_start+1], (3*model_config.embedding_dim,)) * scaling_factor,\n",
        "            },\n",
        "            'attn_out': {\n",
        "                'weight': jax.random.normal(keys[block_key_start+2], (model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "                'bias': jax.random.normal(keys[block_key_start+3], (model_config.embedding_dim,)) * scaling_factor,\n",
        "            },\n",
        "            'ln1': {\n",
        "                'gamma': jnp.ones((model_config.embedding_dim,)),\n",
        "                'beta': jnp.zeros((model_config.embedding_dim,)),\n",
        "            },\n",
        "            'ln2': {\n",
        "                'gamma': jnp.ones((model_config.embedding_dim,)),\n",
        "                'beta': jnp.zeros((model_config.embedding_dim,)),\n",
        "            },\n",
        "            'ffn_in': {\n",
        "                'weight': jax.random.normal(keys[block_key_start+4], (model_config.embedding_dim, 4*model_config.embedding_dim)) * scaling_factor,\n",
        "                'bias': jax.random.normal(keys[block_key_start+5], (4*model_config.embedding_dim,)) * scaling_factor,\n",
        "            },\n",
        "            'ffn_out': {\n",
        "                'weight': jax.random.normal(keys[block_key_start+6], (4*model_config.embedding_dim, model_config.embedding_dim)) * scaling_factor,\n",
        "                'bias': jax.random.normal(keys[block_key_start+7], (model_config.embedding_dim,)) * scaling_factor,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "12gOUvbjYBVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets first define the structure of what we're creating\n",
        "# assume the transformer_block function is already implemented\n",
        "# none of this should be new, we're just adding in the logic to pass the input through multiple blocks\n",
        "\n",
        "\"\"\"\n",
        "def multilayer_transformer(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = ...\n",
        "\n",
        "    token_embeds = ...\n",
        "    pos_embeds = ...\n",
        "    x = ...\n",
        "\n",
        "    causal_mask = ...\n",
        "\n",
        "    for i in range(model_config.n_blocks):\n",
        "        x = ...\n",
        "\n",
        "    x = ...\n",
        "\n",
        "    token_logits = ...\n",
        "\n",
        "    return token_logits\n",
        "\"\"\"\n",
        "\n",
        "def multilayer_transformer(model_params: Dict, model_config: ModelConfig, tokens: jax.Array) -> jax.Array:\n",
        "    seq_len = tokens.shape[0]\n",
        "\n",
        "    token_embeds = model_params['token_embedding'][tokens]\n",
        "    pos_embeds = model_params['positional_embedding'][:seq_len]\n",
        "    x = token_embeds + pos_embeds\n",
        "\n",
        "    causal_mask = create_causal_mask(seq_len)\n",
        "\n",
        "    for i in range(model_config.n_blocks):\n",
        "        x = transformer_block(x, model_params[f'block_{i}'], model_config, causal_mask)\n",
        "\n",
        "    x = layer_norm(x, model_params['lnf']['gamma'], model_params['lnf']['beta'])\n",
        "\n",
        "    token_logits = jnp.dot(x, model_params['output_projection'])\n",
        "\n",
        "    return token_logits"
      ],
      "metadata": {
        "id": "GxyXLGnXXljF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we implemnt the actual block\n",
        "# again, nothing you haven't seen before\n",
        "\n",
        "\"\"\"\n",
        "def transformer_block(x: jax.Array, block_params: Dict, model_config: ModelConfig, causal_mask: jax.Array):\n",
        "\n",
        "    residual = ...\n",
        "\n",
        "    normed_x = ...\n",
        "\n",
        "    context = ...\n",
        "\n",
        "    context = ...\n",
        "\n",
        "    context_residual = ...\n",
        "    normed_context = ...\n",
        "\n",
        "    enhanced_context = ...\n",
        "\n",
        "    enhanced_context = ...\n",
        "\n",
        "    return enhanced_context\n",
        "\"\"\"\n",
        "\n",
        "def transformer_block(x: jax.Array, block_params: Dict, model_config: ModelConfig, causal_mask: jax.Array):\n",
        "\n",
        "    residual = x\n",
        "\n",
        "    normed_x = layer_norm(x, block_params['ln1']['gamma'], block_params['ln1']['beta'])\n",
        "\n",
        "    context = multi_head_attn(normed_x, block_params['attn_in']['weight'], block_params['attn_in']['bias'],\n",
        "                              block_params['attn_out']['weight'], block_params['attn_out']['bias'],\n",
        "                              model_config.n_head, causal_mask)\n",
        "\n",
        "    context = context + residual\n",
        "\n",
        "    context_residual = context\n",
        "    normed_context = layer_norm(context, block_params['ln2']['gamma'], block_params['ln2']['beta'])\n",
        "\n",
        "    enhanced_context = ffn(normed_context, block_params['ffn_in']['weight'], block_params['ffn_in']['bias'],\n",
        "                           block_params['ffn_out']['weight'], block_params['ffn_out']['bias'])\n",
        "\n",
        "    enhanced_context = enhanced_context + context_residual\n",
        "\n",
        "    return enhanced_context"
      ],
      "metadata": {
        "id": "o2noglQ9Upw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DICT = {\n",
        "    'bigram_model': bigram_model,\n",
        "    'bigram_embed': bigram_embed,\n",
        "    'causal_embed': causal_embed,\n",
        "    'causal_pos': causal_pos,\n",
        "    'attentive_model': attentive_model,\n",
        "    'attentive_ffn': attentive_ffn,\n",
        "    'attentive_ffn_ln': attentive_ffn_ln,\n",
        "    'attentive_ffn_ln_res': attentive_ffn_ln_res,\n",
        "    'multilayer_transformer': multilayer_transformer,\n",
        "\n",
        "}\n",
        "\n",
        "transformer_config = ModelConfig(vocab_size=95, embedding_dim=32, context_len=128, n_head=4, n_blocks=12)\n",
        "multilayer_transformer_params = init_multilayer_transformer_params(key, transformer_config)\n",
        "trained_multilayer_transformer_params = train(multilayer_transformer_params, transformer_config, 'multilayer_transformer', train_tokens, adamw_config, train_config)\n",
        "compare_params(prompt, transformer_config, 'multilayer_transformer', multilayer_transformer_params, trained_multilayer_transformer_params)"
      ],
      "metadata": {
        "id": "nJTjRq5sT8Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sampling hyper parameters"
      ],
      "metadata": {
        "id": "bo9-Gjqcwmrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# thats all for the model architecture part of gpt2\n",
        "# but theres a couple more things we can do with generation and tokenization\n",
        "\n",
        "# right now we sample just using temperature and softmax\n",
        "# even with temperature control\n",
        "# we still consider literally every token\n",
        "# this is kind of a pain, because we might generate a super unlikely token\n",
        "# even with low temperature\n",
        "# so we want a way to only select some 'reasonable' tokens\n",
        "\n",
        "# the way that gpt-2 did this was a method called top k\n",
        "# exactly as it sounds, it restricts the sample space to only the top k tokens"
      ],
      "metadata": {
        "id": "mvbR5HFUUEGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top k is nice, but it doesnt really account for differently shaped distributions\n",
        "# at some generations, we might have a lot of different highly probable choices\n",
        "# that are greater than k\n",
        "# instead of the discrete cutoff of k\n",
        "# we could instead use a cutoff of some amount of cumulative probability\n",
        "# and only sample tokens within that space\n",
        "# this is known as top p, or nucleus sampling\n",
        "# this is because it samples from the nucleus or core of the probability distribution"
      ],
      "metadata": {
        "id": "358fqtMuUGYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another sampling filter we can add is min_p\n",
        "# it makes sure we only select logits above some minimum threshold\n",
        "# this is especially helpful when temperatures are high, like above 1\n",
        "# usually this can result in creative but unintelligible output\n",
        "# but min_p helps control it"
      ],
      "metadata": {
        "id": "-bEZ89m5UGve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def generate(params: Dict, model_config: ModelConfig, model_name: str, tokens: jax.Array, max_new: int, key: jax.random.PRNGKey, temp=1.0, top_k=None, top_p=None, min_p=None):\n",
        "\n",
        "    gen_tokens = jnp.array([], dtype=jnp.int32)\n",
        "    cur_pos = 0\n",
        "\n",
        "    # you'll probably need to use jnp where, jnp take along axis, jnp maximum, etc\n",
        "    # one neat way to do this is to update a logit cutoff value with each of the filtering steps\n",
        "\n",
        "    def filter_logits(logits, top_k=None, top_p=None, min_p=0.1):\n",
        "        ...\n",
        "\n",
        "        # Initialize cutoff with the minimum logit value\n",
        "        cutoff = ...\n",
        "\n",
        "        if min_p is not None:\n",
        "            min_p_mask = ...\n",
        "            cutoff = ...\n",
        "\n",
        "        if top_k is not None:\n",
        "            top_k_cutoff = ...\n",
        "            cutoff = ...\n",
        "\n",
        "        if top_p is not None:\n",
        "            top_p_mask = ...\n",
        "            cutoff = ...\n",
        "\n",
        "        return jnp.where(logits < cutoff, -jnp.inf, logits)\n",
        "\n",
        "    while cur_pos < max_new:\n",
        "        key, subkey = jax.random.split(key, 2)\n",
        "        logits = MODEL_DICT[model_name](params, model_config, tokens)\n",
        "        last_token_logit = logits[-1:]\n",
        "        scaled_logits = last_token_logit / temp\n",
        "        filtered_logits = filter_logits(scaled_logits, top_k, top_p)\n",
        "\n",
        "        next_token = jax.random.categorical(subkey, filtered_logits, shape=(1,))\n",
        "        gen_tokens = jnp.concatenate((gen_tokens, next_token))\n",
        "        tokens = jnp.concatenate((tokens, next_token))\n",
        "        cur_pos += 1\n",
        "\n",
        "    return gen_tokens\n",
        "\"\"\"\n",
        "\n",
        "def generate(params: Dict, model_config: ModelConfig, model_name: str, tokens: jax.Array, max_new: int, key: jax.random.PRNGKey, temp=1.0, top_k=None, top_p=None, min_p=None):\n",
        "\n",
        "    gen_tokens = jnp.array([], dtype=jnp.int32)\n",
        "    cur_pos = 0\n",
        "\n",
        "    def filter_logits(logits, top_k=None, top_p=None, min_p=0.1):\n",
        "        probs = jax.nn.softmax(logits, axis=-1)\n",
        "        sorted_indices = jnp.argsort(logits, axis=-1)[:, ::-1]\n",
        "        sorted_logits = jnp.take_along_axis(logits, sorted_indices, axis=-1)\n",
        "        cumulative_probs = jnp.cumsum(jnp.take_along_axis(probs, sorted_indices, axis=-1), axis=-1)\n",
        "\n",
        "        cutoff = jnp.min(logits, axis=-1, keepdims=True)\n",
        "\n",
        "        if min_p is not None:\n",
        "            min_p_mask = probs >= min_p\n",
        "            cutoff = jnp.where(min_p_mask, cutoff, -jnp.inf)\n",
        "\n",
        "        if top_k is not None:\n",
        "            top_k_cutoff = sorted_logits[:, top_k-1:top_k]\n",
        "            cutoff = jnp.maximum(cutoff, top_k_cutoff)\n",
        "\n",
        "        if top_p is not None:\n",
        "            top_p_mask = cumulative_probs <= top_p\n",
        "            dynamic_cutoff = jnp.where(top_p_mask, sorted_logits, -jnp.inf)\n",
        "            cutoff = jnp.maximum(cutoff, jnp.max(dynamic_cutoff, axis=-1, keepdims=True))\n",
        "\n",
        "        return jnp.where(logits < cutoff, -jnp.inf, logits)\n",
        "\n",
        "    while cur_pos < max_new:\n",
        "        key, subkey = jax.random.split(key, 2)\n",
        "        logits = MODEL_DICT[model_name](params, model_config, tokens)\n",
        "        last_token_logit = logits[-1:]\n",
        "        scaled_logits = last_token_logit / temp\n",
        "        filtered_logits = filter_logits(scaled_logits, top_k, top_p)\n",
        "\n",
        "        next_token = jax.random.categorical(subkey, filtered_logits, shape=(1,))\n",
        "        gen_tokens = jnp.concatenate((gen_tokens, next_token))\n",
        "        tokens = jnp.concatenate((tokens, next_token))\n",
        "        cur_pos += 1\n",
        "\n",
        "    return gen_tokens"
      ],
      "metadata": {
        "id": "80ag8tC3UIre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use this to play around with the parameters, notice how they affect the generations\n",
        "\n",
        "tokenized_prompt = jnp.array(jit_tokenizer.encode(preprocess_text(prompt)), dtype=jnp.int32)\n",
        "generated_tokens = generate(trained_multilayer_transformer_params, transformer_config, 'multilayer_transformer', tokens=tokenized_prompt, max_new=10, temp=1.5, min_p=0.05, top_k=40, top_p=0.5, key=jax.random.PRNGKey(0))\n",
        "generated_text = jit_tokenizer.decode(generated_tokens)\n",
        "print(jit_tokenizer.decode(tokenized_prompt))\n",
        "print(prompt + generated_text)"
      ],
      "metadata": {
        "id": "0igc-gfYijk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### byte pair tokenization"
      ],
      "metadata": {
        "id": "QCC-RjBswoGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# right now we tokenize with every single character\n",
        "# but this becomes quite inefficient quickly\n",
        "# one of the easiest ways to understand this is the context length\n",
        "# since our model has a fixed context length,\n",
        "# if each of our tokens is a character, we won't get long enough generations\n",
        "\n",
        "# making each token a word would be pretty tough too\n",
        "# for one, our model would never be able to say a word it hasnt seen before\n",
        "# this limits the models creativity and forces us to have very very strong training data\n",
        "# so we want some way to get between a full word and a character\n",
        "\n",
        "# our problem is basically a data compression problem\n",
        "# words might be overly compressive, and each character is under compressive\n",
        "# lets think of each character as carrying some amount of information, on its own\n",
        "# suppose each character is a byte, from 0-255\n",
        "# if we were to inspect our training data\n",
        "# we might find that some bytes tend to be paired with others\n",
        "# rather than having to predict both bytes\n",
        "# why not develop a way to predict both of those bytes at the same time?\n",
        "# if we see a very common pair of bytes, we can replace it with a new byte that hasnt been used, say 256 in our example\n",
        "# we can do this as many times as we want on our training data, until we think we've achieved sufficient compression\n"
      ],
      "metadata": {
        "id": "mgPOvMzYUK7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rather than using ASCII as we have above\n",
        "# we'll use unicode, so we can support emojis and such\n",
        "text = \"This is an example text for BPE tokenization. It includes some unicode characters like 你好 and émoji 😊.\"\n",
        "list(text.encode('utf-8'))[:20]"
      ],
      "metadata": {
        "id": "D627fE5YUMk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so to compress our data, we'll want to find frequent token pairs\n",
        "# and greedily merge them\n",
        "# and maintain a mapping from the original bytes to our merged indexes\n",
        "# first lets write a function to count token pairs\n",
        "\n",
        "\"\"\"\n",
        "def count_token_pairs(tokens):\n",
        "    token_pairs = {}\n",
        "    ...\n",
        "    return token_pairs\n",
        "\"\"\"\n",
        "\n",
        "def count_token_pairs(tokens):\n",
        "    token_pairs = {}\n",
        "    for i in range(len(tokens)-1):\n",
        "        pair = (tokens[i], tokens[i+1])\n",
        "        token_pairs[pair] = token_pairs.get(pair, 0) + 1\n",
        "    return token_pairs\n",
        "\n",
        "tokens = list(text.encode('utf-8'))\n",
        "token_pairs = count_token_pairs(tokens)\n",
        "sorted_pairs = sorted(((v,k) for k,v in token_pairs.items()), reverse=True)\n",
        "[(chr(x[1][0]), chr(x[1][1])) for x in sorted_pairs[:5]] # print out top 5"
      ],
      "metadata": {
        "id": "791f7AArUO1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now anytime we see the top pair count, swap it out for new max value\n",
        "# [66, 121, 116, 101, 32, 112, 97, 105, 114, 32]\n",
        "# [66, 121, 116, NEW, 112, 97, 105, 114, 32]\n",
        "\"\"\"\n",
        "def merge_pair(tokens, top_pair, new_id):\n",
        "    new_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == top_pair:\n",
        "            new_tokens.append(new_id)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "    return new_tokens\n",
        "\"\"\"\n",
        "\n",
        "def merge_pair(tokens, top_pair, new_id):\n",
        "    new_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if i < len(tokens) - 1 and (tokens[i], tokens[i+1]) == top_pair:\n",
        "            new_tokens.append(new_id)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "    return new_tokens\n",
        "\n",
        "top_pair = sorted_pairs[0][1]\n",
        "print(top_pair)\n",
        "new_tokens = merge_pair(tokens, top_pair, new_id = 256)\n",
        "print(new_tokens[:20])"
      ],
      "metadata": {
        "id": "y1mE2edaURJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to encode new text\n",
        "# we need to know the merges we performed, in order\n",
        "# so that we can perform them correctly when encoding new text\n",
        "\n",
        "# to decode text\n",
        "# we'll need a dictionary that maps integer ids back into bytes objects to decode with utf8\n",
        "# we'll call this id_to_token\n",
        "\n",
        "\n",
        "# let's write the encode/decode functions now\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def encode(text, merges):\n",
        "    tokens = list(text.encode('utf-8'))\n",
        "    # apply merges\n",
        "    return tokens\n",
        "\n",
        "def decode(encoded_tokens, id_to_token):\n",
        "    # use id_to_token and convert to a bytes object with b''.join()\n",
        "    return\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def encode(text, merges):\n",
        "    tokens = list(text.encode('utf-8'))\n",
        "    for pair, new_id in merges:\n",
        "        tokens = merge_pair(tokens, pair, new_id)\n",
        "    return tokens\n",
        "\n",
        "def decode(encoded_tokens, id_to_token):\n",
        "    return b''.join(id_to_token[token] for token in encoded_tokens).decode('utf-8', errors='replace')\n"
      ],
      "metadata": {
        "id": "KzoK3vRjktrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now to learn the byte pair encoding, and create the id_to_token and merges\n",
        "# we should"
      ],
      "metadata": {
        "id": "tCR3w2CenCfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to learn the byte pair encoding\n",
        "# we'll define a target vocab size\n",
        "# the amount of integer ids we'll end up with that our model can use\n",
        "# first we list/encode the text\n",
        "# initialize the mappings from bytes objects to tokens\n",
        "# and initialize the next_id parameter\n",
        "# which we'll have to increment\n",
        "# while our vocabulary size isn't at the target\n",
        "# we count pairs\n",
        "# exiting early if there are no more pairs to merge\n",
        "# then we take the top pair\n",
        "# make the new token\n",
        "# create the mappings between that new token and the ids\n",
        "# add this information to merges\n",
        "# merge the tokens\n",
        "# and continue\n",
        "# there are some cool opportunities to optimize this\n",
        "# but the solutions only implement the brute force method for now\n",
        "\n",
        "\"\"\"\n",
        "def learn_bpe(text, target_vocab_size):\n",
        "    tokens = list(text.encode('utf-8'))\n",
        "    token_to_id = {i: bytes([i]) for i in range(256)}  # Initialize with byte tokens\n",
        "    id_to_token = {i: bytes([i]) for i in range(256)}  # Reverse mapping\n",
        "    next_id = 256\n",
        "    merges = []\n",
        "\n",
        "    while len(token_to_id) < target_vocab_size:\n",
        "        ...\n",
        "\n",
        "    return token_to_id, id_to_token, merges\n",
        "\"\"\"\n",
        "\n",
        "def learn_bpe(text, target_vocab_size):\n",
        "    tokens = list(text.encode('utf-8'))\n",
        "    token_to_id = {i: bytes([i]) for i in range(256)}  # Initialize with byte tokens\n",
        "    id_to_token = {i: bytes([i]) for i in range(256)}  # Reverse mapping\n",
        "    next_id = 256\n",
        "    merges = []\n",
        "\n",
        "    while len(token_to_id) < target_vocab_size:\n",
        "        if len(token_to_id) % 100 == 0:\n",
        "            print('vocab at: ', len(token_to_id))\n",
        "\n",
        "        token_pairs = count_token_pairs(tokens)\n",
        "        if not token_pairs:\n",
        "            print(\"no more pairs to merge. current vocab size:\", len(token_to_id))\n",
        "            break\n",
        "\n",
        "        top_pair = max(token_pairs, key=token_pairs.get)\n",
        "        new_token = id_to_token[top_pair[0]] + id_to_token[top_pair[1]]\n",
        "        token_to_id[new_token] = next_id\n",
        "        id_to_token[next_id] = new_token\n",
        "        merges.append((top_pair, next_id))\n",
        "\n",
        "        tokens = merge_pair(tokens, top_pair, next_id)\n",
        "        next_id += 1\n",
        "\n",
        "        if len(token_to_id) >= target_vocab_size:\n",
        "            break\n",
        "\n",
        "    return token_to_id, id_to_token, merges\n",
        "\n",
        "\n",
        "\n",
        "target_vocab_size = 1256\n",
        "token_to_id, id_to_token, merges = learn_bpe(train_text[:10000], target_vocab_size) # running the full tokenization takes a long time"
      ],
      "metadata": {
        "id": "liqMGiLaUSQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = encode(train_text[:1000], merges)\n",
        "\n",
        "compression_ratio = len(encoded) / len(train_text[:1000])\n",
        "print(f\"compression: ({len(train_text[:1000])}) -> ({len(encoded)}): ({compression_ratio:.2f})\")"
      ],
      "metadata": {
        "id": "EWNLQATdUUwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### full gpt-2 weights"
      ],
      "metadata": {
        "id": "QpHhqlF1wuU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will load the actual gpt2 tokenizer and weights\n",
        "# we've already written everything above - so now we can just skip to the finished model\n",
        "\n",
        "# we'll import from hugging face\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "wL7v3E6VUXZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the tokenizer is as simple as\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # we call this similarly to how we made our other tokenizers\n",
        "\n",
        "# the model itself is a bit more complicated\n",
        "gpt2 = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "gpt2"
      ],
      "metadata": {
        "id": "BBQDx9OrrSzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as you can see all of the stuff in the model is the stuff that we've already made!\n",
        "# now is the part where you would write the unpacking of the model parameters into the structure we've defined\n",
        "# this is honestly kind of tedious so here's the code"
      ],
      "metadata": {
        "id": "cj_Ox1X9ru6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_gpt2_params(model_name=\"gpt2\"):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "    config = model.config\n",
        "    model_config = ModelConfig(\n",
        "        vocab_size=config.vocab_size,\n",
        "        embedding_dim=config.n_embd,\n",
        "        context_len=config.n_positions,\n",
        "        n_head=config.n_head,\n",
        "        n_blocks=config.n_layer\n",
        "    )\n",
        "\n",
        "    converted_params = {}\n",
        "\n",
        "    converted_params['token_embedding'] = jnp.array(model.transformer.wte.weight.detach().numpy())\n",
        "\n",
        "    converted_params['positional_embedding'] = jnp.array(model.transformer.wpe.weight.detach().numpy())\n",
        "\n",
        "    converted_params['output_projection'] = jnp.array(model.lm_head.weight.detach().numpy().T)\n",
        "\n",
        "    converted_params['lnf'] = {\n",
        "        'gamma': jnp.array(model.transformer.ln_f.weight.detach().numpy()),\n",
        "        'beta': jnp.array(model.transformer.ln_f.bias.detach().numpy())\n",
        "    }\n",
        "\n",
        "    # Convert transformer blocks\n",
        "    for i in range(model_config.n_blocks):\n",
        "        block = model.transformer.h[i]\n",
        "        converted_block = {\n",
        "            'attn_in': {\n",
        "                'weight': jnp.array(block.attn.c_attn.weight.detach().numpy()),\n",
        "                'bias': jnp.array(block.attn.c_attn.bias.detach().numpy())\n",
        "            },\n",
        "            'attn_out': {\n",
        "                'weight': jnp.array(block.attn.c_proj.weight.detach().numpy()),\n",
        "                'bias': jnp.array(block.attn.c_proj.bias.detach().numpy())\n",
        "            },\n",
        "            'ln1': {\n",
        "                'gamma': jnp.array(block.ln_1.weight.detach().numpy()),\n",
        "                'beta': jnp.array(block.ln_1.bias.detach().numpy())\n",
        "            },\n",
        "            'ln2': {\n",
        "                'gamma': jnp.array(block.ln_2.weight.detach().numpy()),\n",
        "                'beta': jnp.array(block.ln_2.bias.detach().numpy())\n",
        "            },\n",
        "            'ffn_in': {\n",
        "                'weight': jnp.array(block.mlp.c_fc.weight.detach().numpy()),\n",
        "                'bias': jnp.array(block.mlp.c_fc.bias.detach().numpy())\n",
        "            },\n",
        "            'ffn_out': {\n",
        "                'weight': jnp.array(block.mlp.c_proj.weight.detach().numpy()),\n",
        "                'bias': jnp.array(block.mlp.c_proj.bias.detach().numpy())\n",
        "            },\n",
        "        }\n",
        "\n",
        "        converted_params[f'block_{i}'] = converted_block\n",
        "\n",
        "    return converted_params, model_config\n",
        "\n",
        "# usage example\n",
        "gpt2_params, gpt2_config = load_gpt2_params()"
      ],
      "metadata": {
        "id": "FtsnX8u9UZIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally: gpt2\n",
        "\n",
        "prompt = 'the short, hilarious tweet from gpt2 read as follows:'\n",
        "\n",
        "tokenized_prompt = jnp.array(gpt2_tokenizer.encode(prompt), dtype=jnp.int32)\n",
        "generated_tokens = generate(gpt2_params, transformer_config, 'multilayer_transformer', tokens=tokenized_prompt, max_new=20, temp=1.5, min_p=0.05, top_k=40, top_p=0.5, key=jax.random.PRNGKey(0))\n",
        "generated_text = gpt2_tokenizer.decode(generated_tokens)\n",
        "print(gpt2_tokenizer.decode(tokenized_prompt))\n",
        "print(prompt + generated_text)"
      ],
      "metadata": {
        "id": "vv5FOuADUbxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hopefully this was instructive! the author would love your feedback as a github issue or twitter dm at @arb8020"
      ],
      "metadata": {
        "id": "CH5FKY1KwUgD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}